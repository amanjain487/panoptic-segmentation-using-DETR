{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dataset Creation",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amanjain487/panoptic-segmentation-using-DETR/blob/anubhav/Dataset_Creation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Dataset for Construction Material + COCO Classes\n",
        "\n",
        "## Following Operations are performed using this colab file\n",
        "- Given, construction dataset - things in our case\n",
        "- use pretrained DETR panoptic model to predict stuff and things for all given images\n",
        "- Add all stuff predictions as ground truth for our dataset and all things predictions as misc stuff class in ground truth\n",
        "- Test - Train split "
      ],
      "metadata": {
        "id": "6_emsoUYPHlW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Requirements and Prepare the Notebook"
      ],
      "metadata": {
        "id": "XDPeEwMJRK7d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STC41cG3O35Z",
        "outputId": "ec3413c9-e21c-4c92-a57f-fe8d30a0150e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch 1.10.0+cu111 _CudaDeviceProperties(name='Tesla K80', major=3, minor=7, total_memory=11441MB, multi_processor_count=13)\n",
            "Collecting git+https://github.com/cocodataset/panopticapi.git\n",
            "  Cloning https://github.com/cocodataset/panopticapi.git to /tmp/pip-req-build-x_08amhc\n",
            "  Running command git clone -q https://github.com/cocodataset/panopticapi.git /tmp/pip-req-build-x_08amhc\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from panopticapi==0.1) (1.21.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from panopticapi==0.1) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import glob\n",
        "import torch\n",
        "import os\n",
        "\n",
        "from IPython.display import Image, clear_output \n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('PyTorch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n",
        "\n",
        "!pip install git+https://github.com/cocodataset/panopticapi.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Drive \n",
        "\n",
        "We need drive access for the following things\n",
        "- Our dataset is stored in drive - to access the dataset\n",
        "- To extract original dataset directly in drive - and use them for groud truth creation\n",
        "- Create ground truths directly in drive"
      ],
      "metadata": {
        "id": "aJdFx4jtP_sA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9-F3aDVO5IE",
        "outputId": "22d1b117-defd-4981-c73b-315affa03de7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unzip the given original dataset\n",
        "\n",
        "- The dataset is uploaded in zip format\n",
        "- Unzip the dataset in drive itself\n",
        "- Once, unzipped, delete the zip file, to manage drive's limited space"
      ],
      "metadata": {
        "id": "3J69eUbVQJur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.chdir(\"/content/drive/MyDrive/Panoptic Segmentation using DETR/Original Dataset\")\n",
        "\n",
        "# import zipfile\n",
        "# !unzip construction_materials_dataset.zip \n",
        "# os.remove(\"construction_materials_dataset.zip\")"
      ],
      "metadata": {
        "id": "LDZRdzkBP-qK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone official DETR Repo to Drive\n",
        "- We will be passing our dataset images through official pre-trained DETR model by Facebook for getting predictions of COCO Classes\n",
        "- The output of DETR model will have lot of predictions ranging from 0% confidence to 100% confidence\n",
        "- All the predictions with confidence greater than 85% will become our ground truth for COCO classes which will be later combined with `construction materials` annotations which will become our final Dataset. "
      ],
      "metadata": {
        "id": "hlQO1MBCosv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/Panoptic Segmentation using DETR/\")\n",
        "!git clone https://github.com/facebookresearch/detr.git\n",
        "sys.path.append(os.path.join(os.getcwd(), \"detr/\"))\n"
      ],
      "metadata": {
        "id": "4S6Rvq07RF8z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43b7a5c8-10e9-443d-c591-5c9e3f942f7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'detr' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PVbYHuQ0pSJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define COCO Classes\n",
        "\n",
        "- Define exisitng COCO classes which will be predicted by Facebook DETR\n",
        "- Define COCO classes which we will be using\n",
        "- Establish mapping from esciting to new ones"
      ],
      "metadata": {
        "id": "mx-DnLnbqWXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "existing_coco_categories = [\n",
        "    {\"color\": [220, 20, 60], \"isthing\": 1, \"id\": 1, \"name\": \"person\"},\n",
        "    {\"color\": [119, 11, 32], \"isthing\": 1, \"id\": 2, \"name\": \"bicycle\"},\n",
        "    {\"color\": [0, 0, 142], \"isthing\": 1, \"id\": 3, \"name\": \"car\"},\n",
        "    {\"color\": [0, 0, 230], \"isthing\": 1, \"id\": 4, \"name\": \"motorcycle\"},\n",
        "    {\"color\": [106, 0, 228], \"isthing\": 1, \"id\": 5, \"name\": \"airplane\"},\n",
        "    {\"color\": [0, 60, 100], \"isthing\": 1, \"id\": 6, \"name\": \"bus\"},\n",
        "    {\"color\": [0, 80, 100], \"isthing\": 1, \"id\": 7, \"name\": \"train\"},\n",
        "    {\"color\": [0, 0, 70], \"isthing\": 1, \"id\": 8, \"name\": \"truck\"},\n",
        "    {\"color\": [0, 0, 192], \"isthing\": 1, \"id\": 9, \"name\": \"boat\"},\n",
        "    {\"color\": [250, 170, 30], \"isthing\": 1, \"id\": 10, \"name\": \"traffic light\"},\n",
        "    {\"color\": [100, 170, 30], \"isthing\": 1, \"id\": 11, \"name\": \"fire hydrant\"},\n",
        "    {\"color\": [220, 220, 0], \"isthing\": 1, \"id\": 13, \"name\": \"stop sign\"},\n",
        "    {\"color\": [175, 116, 175], \"isthing\": 1, \"id\": 14, \"name\": \"parking meter\"},\n",
        "    {\"color\": [250, 0, 30], \"isthing\": 1, \"id\": 15, \"name\": \"bench\"},\n",
        "    {\"color\": [165, 42, 42], \"isthing\": 1, \"id\": 16, \"name\": \"bird\"},\n",
        "    {\"color\": [255, 77, 255], \"isthing\": 1, \"id\": 17, \"name\": \"cat\"},\n",
        "    {\"color\": [0, 226, 252], \"isthing\": 1, \"id\": 18, \"name\": \"dog\"},\n",
        "    {\"color\": [182, 182, 255], \"isthing\": 1, \"id\": 19, \"name\": \"horse\"},\n",
        "    {\"color\": [0, 82, 0], \"isthing\": 1, \"id\": 20, \"name\": \"sheep\"},\n",
        "    {\"color\": [120, 166, 157], \"isthing\": 1, \"id\": 21, \"name\": \"cow\"},\n",
        "    {\"color\": [110, 76, 0], \"isthing\": 1, \"id\": 22, \"name\": \"elephant\"},\n",
        "    {\"color\": [174, 57, 255], \"isthing\": 1, \"id\": 23, \"name\": \"bear\"},\n",
        "    {\"color\": [199, 100, 0], \"isthing\": 1, \"id\": 24, \"name\": \"zebra\"},\n",
        "    {\"color\": [72, 0, 118], \"isthing\": 1, \"id\": 25, \"name\": \"giraffe\"},\n",
        "    {\"color\": [255, 179, 240], \"isthing\": 1, \"id\": 27, \"name\": \"backpack\"},\n",
        "    {\"color\": [0, 125, 92], \"isthing\": 1, \"id\": 28, \"name\": \"umbrella\"},\n",
        "    {\"color\": [209, 0, 151], \"isthing\": 1, \"id\": 31, \"name\": \"handbag\"},\n",
        "    {\"color\": [188, 208, 182], \"isthing\": 1, \"id\": 32, \"name\": \"tie\"},\n",
        "    {\"color\": [0, 220, 176], \"isthing\": 1, \"id\": 33, \"name\": \"suitcase\"},\n",
        "    {\"color\": [255, 99, 164], \"isthing\": 1, \"id\": 34, \"name\": \"frisbee\"},\n",
        "    {\"color\": [92, 0, 73], \"isthing\": 1, \"id\": 35, \"name\": \"skis\"},\n",
        "    {\"color\": [133, 129, 255], \"isthing\": 1, \"id\": 36, \"name\": \"snowboard\"},\n",
        "    {\"color\": [78, 180, 255], \"isthing\": 1, \"id\": 37, \"name\": \"sports ball\"},\n",
        "    {\"color\": [0, 228, 0], \"isthing\": 1, \"id\": 38, \"name\": \"kite\"},\n",
        "    {\"color\": [174, 255, 243], \"isthing\": 1, \"id\": 39, \"name\": \"baseball bat\"},\n",
        "    {\"color\": [45, 89, 255], \"isthing\": 1, \"id\": 40, \"name\": \"baseball glove\"},\n",
        "    {\"color\": [134, 134, 103], \"isthing\": 1, \"id\": 41, \"name\": \"skateboard\"},\n",
        "    {\"color\": [145, 148, 174], \"isthing\": 1, \"id\": 42, \"name\": \"surfboard\"},\n",
        "    {\"color\": [255, 208, 186], \"isthing\": 1, \"id\": 43, \"name\": \"tennis racket\"},\n",
        "    {\"color\": [197, 226, 255], \"isthing\": 1, \"id\": 44, \"name\": \"bottle\"},\n",
        "    {\"color\": [171, 134, 1], \"isthing\": 1, \"id\": 46, \"name\": \"wine glass\"},\n",
        "    {\"color\": [109, 63, 54], \"isthing\": 1, \"id\": 47, \"name\": \"cup\"},\n",
        "    {\"color\": [207, 138, 255], \"isthing\": 1, \"id\": 48, \"name\": \"fork\"},\n",
        "    {\"color\": [151, 0, 95], \"isthing\": 1, \"id\": 49, \"name\": \"knife\"},\n",
        "    {\"color\": [9, 80, 61], \"isthing\": 1, \"id\": 50, \"name\": \"spoon\"},\n",
        "    {\"color\": [84, 105, 51], \"isthing\": 1, \"id\": 51, \"name\": \"bowl\"},\n",
        "    {\"color\": [74, 65, 105], \"isthing\": 1, \"id\": 52, \"name\": \"banana\"},\n",
        "    {\"color\": [166, 196, 102], \"isthing\": 1, \"id\": 53, \"name\": \"apple\"},\n",
        "    {\"color\": [208, 195, 210], \"isthing\": 1, \"id\": 54, \"name\": \"sandwich\"},\n",
        "    {\"color\": [255, 109, 65], \"isthing\": 1, \"id\": 55, \"name\": \"orange\"},\n",
        "    {\"color\": [0, 143, 149], \"isthing\": 1, \"id\": 56, \"name\": \"broccoli\"},\n",
        "    {\"color\": [179, 0, 194], \"isthing\": 1, \"id\": 57, \"name\": \"carrot\"},\n",
        "    {\"color\": [209, 99, 106], \"isthing\": 1, \"id\": 58, \"name\": \"hot dog\"},\n",
        "    {\"color\": [5, 121, 0], \"isthing\": 1, \"id\": 59, \"name\": \"pizza\"},\n",
        "    {\"color\": [227, 255, 205], \"isthing\": 1, \"id\": 60, \"name\": \"donut\"},\n",
        "    {\"color\": [147, 186, 208], \"isthing\": 1, \"id\": 61, \"name\": \"cake\"},\n",
        "    {\"color\": [153, 69, 1], \"isthing\": 1, \"id\": 62, \"name\": \"chair\"},\n",
        "    {\"color\": [3, 95, 161], \"isthing\": 1, \"id\": 63, \"name\": \"couch\"},\n",
        "    {\"color\": [163, 255, 0], \"isthing\": 1, \"id\": 64, \"name\": \"potted plant\"},\n",
        "    {\"color\": [119, 0, 170], \"isthing\": 1, \"id\": 65, \"name\": \"bed\"},\n",
        "    {\"color\": [0, 182, 199], \"isthing\": 1, \"id\": 67, \"name\": \"dining table\"},\n",
        "    {\"color\": [0, 165, 120], \"isthing\": 1, \"id\": 70, \"name\": \"toilet\"},\n",
        "    {\"color\": [183, 130, 88], \"isthing\": 1, \"id\": 72, \"name\": \"tv\"},\n",
        "    {\"color\": [95, 32, 0], \"isthing\": 1, \"id\": 73, \"name\": \"laptop\"},\n",
        "    {\"color\": [130, 114, 135], \"isthing\": 1, \"id\": 74, \"name\": \"mouse\"},\n",
        "    {\"color\": [110, 129, 133], \"isthing\": 1, \"id\": 75, \"name\": \"remote\"},\n",
        "    {\"color\": [166, 74, 118], \"isthing\": 1, \"id\": 76, \"name\": \"keyboard\"},\n",
        "    {\"color\": [219, 142, 185], \"isthing\": 1, \"id\": 77, \"name\": \"cell phone\"},\n",
        "    {\"color\": [79, 210, 114], \"isthing\": 1, \"id\": 78, \"name\": \"microwave\"},\n",
        "    {\"color\": [178, 90, 62], \"isthing\": 1, \"id\": 79, \"name\": \"oven\"},\n",
        "    {\"color\": [65, 70, 15], \"isthing\": 1, \"id\": 80, \"name\": \"toaster\"},\n",
        "    {\"color\": [127, 167, 115], \"isthing\": 1, \"id\": 81, \"name\": \"sink\"},\n",
        "    {\"color\": [59, 105, 106], \"isthing\": 1, \"id\": 82, \"name\": \"refrigerator\"},\n",
        "    {\"color\": [142, 108, 45], \"isthing\": 1, \"id\": 84, \"name\": \"book\"},\n",
        "    {\"color\": [196, 172, 0], \"isthing\": 1, \"id\": 85, \"name\": \"clock\"},\n",
        "    {\"color\": [95, 54, 80], \"isthing\": 1, \"id\": 86, \"name\": \"vase\"},\n",
        "    {\"color\": [128, 76, 255], \"isthing\": 1, \"id\": 87, \"name\": \"scissors\"},\n",
        "    {\"color\": [201, 57, 1], \"isthing\": 1, \"id\": 88, \"name\": \"teddy bear\"},\n",
        "    {\"color\": [246, 0, 122], \"isthing\": 1, \"id\": 89, \"name\": \"hair drier\"},\n",
        "    {\"color\": [191, 162, 208], \"isthing\": 1, \"id\": 90, \"name\": \"toothbrush\"},\n",
        "    {\"color\": [255, 255, 128], \"isthing\": 0, \"id\": 92, \"name\": \"banner\"},\n",
        "    {\"color\": [147, 211, 203], \"isthing\": 0, \"id\": 93, \"name\": \"blanket\"},\n",
        "    {\"color\": [150, 100, 100], \"isthing\": 0, \"id\": 95, \"name\": \"bridge\"},\n",
        "    {\"color\": [168, 171, 172], \"isthing\": 0, \"id\": 100, \"name\": \"cardboard\"},\n",
        "    {\"color\": [146, 112, 198], \"isthing\": 0, \"id\": 107, \"name\": \"counter\"},\n",
        "    {\"color\": [210, 170, 100], \"isthing\": 0, \"id\": 109, \"name\": \"curtain\"},\n",
        "    {\"color\": [92, 136, 89], \"isthing\": 0, \"id\": 112, \"name\": \"door-stuff\"},\n",
        "    {\"color\": [218, 88, 184], \"isthing\": 0, \"id\": 118, \"name\": \"floor-wood\"},\n",
        "    {\"color\": [241, 129, 0], \"isthing\": 0, \"id\": 119, \"name\": \"flower\"},\n",
        "    {\"color\": [217, 17, 255], \"isthing\": 0, \"id\": 122, \"name\": \"fruit\"},\n",
        "    {\"color\": [124, 74, 181], \"isthing\": 0, \"id\": 125, \"name\": \"gravel\"},\n",
        "    {\"color\": [70, 70, 70], \"isthing\": 0, \"id\": 128, \"name\": \"house\"},\n",
        "    {\"color\": [255, 228, 255], \"isthing\": 0, \"id\": 130, \"name\": \"light\"},\n",
        "    {\"color\": [154, 208, 0], \"isthing\": 0, \"id\": 133, \"name\": \"mirror-stuff\"},\n",
        "    {\"color\": [193, 0, 92], \"isthing\": 0, \"id\": 138, \"name\": \"net\"},\n",
        "    {\"color\": [76, 91, 113], \"isthing\": 0, \"id\": 141, \"name\": \"pillow\"},\n",
        "    {\"color\": [255, 180, 195], \"isthing\": 0, \"id\": 144, \"name\": \"platform\"},\n",
        "    {\"color\": [106, 154, 176], \"isthing\": 0, \"id\": 145, \"name\": \"playingfield\"},\n",
        "    {\"color\": [230, 150, 140], \"isthing\": 0, \"id\": 147, \"name\": \"railroad\"},\n",
        "    {\"color\": [60, 143, 255], \"isthing\": 0, \"id\": 148, \"name\": \"river\"},\n",
        "    {\"color\": [128, 64, 128], \"isthing\": 0, \"id\": 149, \"name\": \"road\"},\n",
        "    {\"color\": [92, 82, 55], \"isthing\": 0, \"id\": 151, \"name\": \"roof\"},\n",
        "    {\"color\": [254, 212, 124], \"isthing\": 0, \"id\": 154, \"name\": \"sand\"},\n",
        "    {\"color\": [73, 77, 174], \"isthing\": 0, \"id\": 155, \"name\": \"sea\"},\n",
        "    {\"color\": [255, 160, 98], \"isthing\": 0, \"id\": 156, \"name\": \"shelf\"},\n",
        "    {\"color\": [255, 255, 255], \"isthing\": 0, \"id\": 159, \"name\": \"snow\"},\n",
        "    {\"color\": [104, 84, 109], \"isthing\": 0, \"id\": 161, \"name\": \"stairs\"},\n",
        "    {\"color\": [169, 164, 131], \"isthing\": 0, \"id\": 166, \"name\": \"tent\"},\n",
        "    {\"color\": [225, 199, 255], \"isthing\": 0, \"id\": 168, \"name\": \"towel\"},\n",
        "    {\"color\": [137, 54, 74], \"isthing\": 0, \"id\": 171, \"name\": \"wall-brick\"},\n",
        "    {\"color\": [135, 158, 223], \"isthing\": 0, \"id\": 175, \"name\": \"wall-stone\"},\n",
        "    {\"color\": [7, 246, 231], \"isthing\": 0, \"id\": 176, \"name\": \"wall-tile\"},\n",
        "    {\"color\": [107, 255, 200], \"isthing\": 0, \"id\": 177, \"name\": \"wall-wood\"},\n",
        "    {\"color\": [58, 41, 149], \"isthing\": 0, \"id\": 178, \"name\": \"water-other\"},\n",
        "    {\"color\": [183, 121, 142], \"isthing\": 0, \"id\": 180, \"name\": \"window-blind\"},\n",
        "    {\"color\": [255, 73, 97], \"isthing\": 0, \"id\": 181, \"name\": \"window-other\"},\n",
        "    {\"color\": [107, 142, 35], \"isthing\": 0, \"id\": 184, \"name\": \"tree-merged\"},\n",
        "    {\"color\": [190, 153, 153], \"isthing\": 0, \"id\": 185, \"name\": \"fence-merged\"},\n",
        "    {\"color\": [146, 139, 141], \"isthing\": 0, \"id\": 186, \"name\": \"ceiling-merged\"},\n",
        "    {\"color\": [70, 130, 180], \"isthing\": 0, \"id\": 187, \"name\": \"sky-other-merged\"},\n",
        "    {\"color\": [134, 199, 156], \"isthing\": 0, \"id\": 188, \"name\": \"cabinet-merged\"},\n",
        "    {\"color\": [209, 226, 140], \"isthing\": 0, \"id\": 189, \"name\": \"table-merged\"},\n",
        "    {\"color\": [96, 36, 108], \"isthing\": 0, \"id\": 190, \"name\": \"floor-other-merged\"},\n",
        "    {\"color\": [96, 96, 96], \"isthing\": 0, \"id\": 191, \"name\": \"pavement-merged\"},\n",
        "    {\"color\": [64, 170, 64], \"isthing\": 0, \"id\": 192, \"name\": \"mountain-merged\"},\n",
        "    {\"color\": [152, 251, 152], \"isthing\": 0, \"id\": 193, \"name\": \"grass-merged\"},\n",
        "    {\"color\": [208, 229, 228], \"isthing\": 0, \"id\": 194, \"name\": \"dirt-merged\"},\n",
        "    {\"color\": [206, 186, 171], \"isthing\": 0, \"id\": 195, \"name\": \"paper-merged\"},\n",
        "    {\"color\": [152, 161, 64], \"isthing\": 0, \"id\": 196, \"name\": \"food-other-merged\"},\n",
        "    {\"color\": [116, 112, 0], \"isthing\": 0, \"id\": 197, \"name\": \"building-other-merged\"},\n",
        "    {\"color\": [0, 114, 143], \"isthing\": 0, \"id\": 198, \"name\": \"rock-merged\"},\n",
        "    {\"color\": [102, 102, 156], \"isthing\": 0, \"id\": 199, \"name\": \"wall-other-merged\"},\n",
        "    {\"color\": [250, 141, 255], \"isthing\": 0, \"id\": 200, \"name\": \"rug-merged\"},\n",
        "]\n",
        "\n",
        "categories_for_pp_project = [{'color': [220, 20, 60], 'isthing': 0, 'id': 1, 'name': 'misc'},\n",
        "            {'color': [255, 255, 128], 'isthing': 0, 'id': 2, 'name': 'textile'},\n",
        "            {'color': [150, 100, 100], 'isthing': 0, 'id': 3, 'name': 'building'},\n",
        "            {'color': [168, 171, 172], 'isthing': 0, 'id': 4, 'name': 'rawmaterial'},\n",
        "            {'color': [146, 112, 198], 'isthing': 0, 'id': 5, 'name': 'furniture'},\n",
        "            {'color': [218, 88, 184], 'isthing': 0, 'id': 6, 'name': 'floor'},\n",
        "            {'color': [241, 129, 0], 'isthing': 0, 'id': 7, 'name': 'plant'},\n",
        "            {'color': [217, 17, 255], 'isthing': 0, 'id': 8, 'name': 'food'},\n",
        "            {'color': [124, 74, 181], 'isthing': 0, 'id': 9, 'name': 'ground'},\n",
        "            {'color': [193, 0, 92], 'isthing': 0, 'id': 10, 'name': 'structural'},\n",
        "            {'color': [60, 143, 255], 'isthing': 0, 'id': 11, 'name': 'water'},\n",
        "            {'color': [137, 54, 74], 'isthing': 0, 'id': 12, 'name': 'wall'},\n",
        "            {'color': [183, 121, 142], 'isthing': 0, 'id': 13, 'name': 'window'},\n",
        "            {'color': [146, 139, 141], 'isthing': 0, 'id': 14, 'name': 'ceiling'},\n",
        "            {'color': [70, 130, 180], 'isthing': 0, 'id': 15, 'name': 'sky'},\n",
        "            {'color': [64, 170, 64], 'isthing': 0, 'id': 16, 'name': 'solid'}]\n",
        "\n",
        "old_to_new_category_mapping = {1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1,\n",
        "           19: 1, 20: 1, 21: 1, 22: 1, 23: 1, 24: 1, 25: 1, 27: 1, 28: 1, 31: 1, 32: 1, 33: 1, 34: 1, 35: 1, 36: 1,\n",
        "           37: 1, 38: 1, 39: 1, 40: 1, 41: 1, 42: 1, 43: 1, 44: 1, 46: 1, 47: 1, 48: 1, 49: 1, 50: 1, 51: 1, 52: 1,\n",
        "           53: 1, 54: 1, 55: 1, 56: 1, 57: 1, 58: 1, 59: 1, 60: 1, 61: 1, 62: 1, 63: 1, 64: 1, 65: 1, 67: 1, 70: 1,\n",
        "           72: 1, 73: 1, 74: 1, 75: 1, 76: 1, 77: 1, 78: 1, 79: 1, 80: 1, 81: 1, 82: 1, 84: 1, 85: 1, 86: 1, 87: 1,\n",
        "           88: 1, 89: 1, 90: 1, 92: 2, 93: 2, 95: 3, 100: 4, 107: 5, 109: 2, 112: 5, 118: 6, 119: 7, 122: 8, 125: 9,\n",
        "           128: 3, 130: 5, 133: 5, 138: 10, 141: 2, 144: 9, 145: 9, 147: 9, 148: 11, 149: 9, 151: 3, 154: 9, 155: 11,\n",
        "           156: 5, 159: 9, 161: 5, 166: 3, 168: 2, 171: 12, 175: 12, 176: 12, 177: 12, 178: 11, 180: 13, 181: 13,\n",
        "           184: 7, 185: 10, 186: 14, 187: 15, 188: 5, 189: 5, 190: 6, 191: 9, 192: 16, 193: 7, 194: 9, 195: 4, 196: 8,\n",
        "           197: 3, 198: 16, 199: 12, 200: 2}"
      ],
      "metadata": {
        "id": "ixrocAHwiYxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assign Category Names to COCO classes and \"NA\" to id numbers which are unassigned"
      ],
      "metadata": {
        "id": "lI83x6EOqp8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coco_category_names = ['N/A'] * 201\n",
        "for c in existing_coco_categories:\n",
        "    coco_category_names[c['id']] = c['name']"
      ],
      "metadata": {
        "id": "UYPezJB2pmXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## List of random colours\n",
        "\n",
        "- Existing COCO classes have unique RGB value to represent a category in MASK Image\n",
        "- But, below is the list of random colours which will be assigned to construction classes"
      ],
      "metadata": {
        "id": "eq-ky8kFq0bZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# since we are treating all things as misc and that belongs to single color class, we can use colors of other things\n",
        "available_colors_for_new_categories = [\n",
        "    [119, 11, 32], \n",
        "    [0, 0, 142], \n",
        "    [0, 0, 230], \n",
        "    [106, 0, 228], \n",
        "    [0, 60, 100],\n",
        "    [0, 80, 100], \n",
        "    [0, 0, 70],\n",
        "    [0, 0, 192], \n",
        "    [250, 170, 30], \n",
        "    [100, 170, 30], \n",
        "    [220, 220, 0], \n",
        "    [175, 116, 175], \n",
        "    [250, 0, 30],\n",
        "    [165, 42, 42], \n",
        "    [255, 77, 255], \n",
        "    [0, 226, 252], \n",
        "    [182, 182, 255], \n",
        "    [0, 82, 0], \n",
        "    [120, 166, 157],\n",
        "    [110, 76, 0], \n",
        "    [174, 57, 255], \n",
        "    [199, 100, 0], \n",
        "    [72, 0, 118], \n",
        "    [255, 179, 240], \n",
        "    [0, 125, 92],\n",
        "    [209, 0, 151], \n",
        "    [188, 208, 182], \n",
        "    [0, 220, 176],\n",
        "    [255, 99, 164], \n",
        "    [92, 0, 73], \n",
        "    [133, 129, 255],\n",
        "    [78, 180, 255], \n",
        "    [0, 228, 0], \n",
        "    [174, 255, 243], \n",
        "    [45, 89, 255], \n",
        "    [134, 134, 103], \n",
        "    [145, 148, 174],\n",
        "    [255, 208, 186], \n",
        "    [197, 226, 255], \n",
        "    [171, 134, 1], \n",
        "    [109, 63, 54], \n",
        "    [207, 138, 255], \n",
        "    [151, 0, 95],\n",
        "    [9, 80, 61], \n",
        "    [84, 105, 51], \n",
        "    [74, 65, 105], \n",
        "    [166, 196, 102], \n",
        "    [208, 195, 210], \n",
        "    [255, 109, 65],\n",
        "    [0, 143, 149], \n",
        "    [179, 0, 194], \n",
        "    [209, 99, 106], \n",
        "    [5, 121, 0], \n",
        "    [227, 255, 205]\n",
        "]"
      ],
      "metadata": {
        "id": "zRIYw0wmjIHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Consturcution CLasses\n",
        "- List of all construction classes on which the model will be trained.\n",
        "\n"
      ],
      "metadata": {
        "id": "q1x-s5pKrHlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_categories = [\n",
        "    \"aac_blocks\",\n",
        "    \"adhesives\",\n",
        "    \"ahus\",\n",
        "    \"aluminium_frames_for_false_ceiling\",\n",
        "    \"chiller\",\n",
        "    \"concrete_mixer_machine\",\n",
        "    \"concrete_pump\",\n",
        "    \"control_panel\",\n",
        "    \"cu_piping\",\n",
        "    \"distribution_transformer\",\n",
        "    \"dump_truck_tipper_truck\",\n",
        "    \"emulsion_paint\",\n",
        "    \"enamel_paint\",\n",
        "    \"fine_aggregate\",\n",
        "    \"fire_buckets\",\n",
        "    \"fire_extinguishers\",\n",
        "    \"glass_wool\",\n",
        "    \"grader\",\n",
        "    \"hoist\",\n",
        "    \"hollow_concrete_blocks\",\n",
        "    \"hot_mix_plant\",\n",
        "    \"hydra_crane\",\n",
        "    \"interlocked_switched_socket\",\n",
        "    \"junction_box\",\n",
        "    \"lime\",\n",
        "    \"marble\",\n",
        "    \"metal_primer\",\n",
        "    \"pipe_fittings\",\n",
        "    \"rcc_hume_pipes\",\n",
        "    \"refrigerant_gas\",\n",
        "    \"river_sand\",\n",
        "    \"rmc_batching_plant\",\n",
        "    \"rmu_units\",\n",
        "    \"sanitary_fixtures\",\n",
        "    \"skid_steer_loader\",\n",
        "    \"smoke_detectors\",\n",
        "    \"split_units\",\n",
        "    \"structural_steel_channel\",\n",
        "    \"switch_boards_and_switches\",\n",
        "    \"texture_paint\",\n",
        "    \"threaded_rod\",\n",
        "    \"transit_mixer\",\n",
        "    \"vcb_panel\",\n",
        "    \"vitrified_tiles\",\n",
        "    \"vrf_units\",\n",
        "    \"water_tank\",\n",
        "    \"wheel_loader\",\n",
        "    \"wood_primer\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "lx_DTls_jJoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Categories\n",
        "\n",
        "- Merge existing COCO classes (our format) + Construction classes\n",
        "- Now we have final list of classes on which our model will be trained"
      ],
      "metadata": {
        "id": "tcQjhbmErQIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category_id = 17\n",
        "available_color_id = 0\n",
        "for category in new_categories:\n",
        "    categories_for_pp_project.append({'color': available_colors_for_new_categories[available_color_id], 'isthing': 1, 'id': category_id, 'name': category})\n",
        "    category_id += 1\n",
        "    available_color_id += 1\n",
        "\n",
        "category_to_id = {\n",
        "    category['name']: category['id'] for category in categories_for_pp_project\n",
        "}\n",
        "\n",
        "id_to_category = {\n",
        "    id: name for id, name in category_to_id.items()\n",
        "}\n",
        "id_to_category"
      ],
      "metadata": {
        "id": "TiQW4RQqjXQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f2ca27d-ac43-4dea-92d5-05b9627db67e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'aac_blocks': 17,\n",
              " 'adhesives': 18,\n",
              " 'ahus': 19,\n",
              " 'aluminium_frames_for_false_ceiling': 20,\n",
              " 'building': 3,\n",
              " 'ceiling': 14,\n",
              " 'chiller': 21,\n",
              " 'concrete_mixer_machine': 22,\n",
              " 'concrete_pump': 23,\n",
              " 'control_panel': 24,\n",
              " 'cu_piping': 25,\n",
              " 'distribution_transformer': 26,\n",
              " 'dump_truck_tipper_truck': 27,\n",
              " 'emulsion_paint': 28,\n",
              " 'enamel_paint': 29,\n",
              " 'fine_aggregate': 30,\n",
              " 'fire_buckets': 31,\n",
              " 'fire_extinguishers': 32,\n",
              " 'floor': 6,\n",
              " 'food': 8,\n",
              " 'furniture': 5,\n",
              " 'glass_wool': 33,\n",
              " 'grader': 34,\n",
              " 'ground': 9,\n",
              " 'hoist': 35,\n",
              " 'hollow_concrete_blocks': 36,\n",
              " 'hot_mix_plant': 37,\n",
              " 'hydra_crane': 38,\n",
              " 'interlocked_switched_socket': 39,\n",
              " 'junction_box': 40,\n",
              " 'lime': 41,\n",
              " 'marble': 42,\n",
              " 'metal_primer': 43,\n",
              " 'misc': 1,\n",
              " 'pipe_fittings': 44,\n",
              " 'plant': 7,\n",
              " 'rawmaterial': 4,\n",
              " 'rcc_hume_pipes': 45,\n",
              " 'refrigerant_gas': 46,\n",
              " 'river_sand': 47,\n",
              " 'rmc_batching_plant': 48,\n",
              " 'rmu_units': 49,\n",
              " 'sanitary_fixtures': 50,\n",
              " 'skid_steer_loader': 51,\n",
              " 'sky': 15,\n",
              " 'smoke_detectors': 52,\n",
              " 'solid': 16,\n",
              " 'split_units': 53,\n",
              " 'structural': 10,\n",
              " 'structural_steel_channel': 54,\n",
              " 'switch_boards_and_switches': 55,\n",
              " 'textile': 2,\n",
              " 'texture_paint': 56,\n",
              " 'threaded_rod': 57,\n",
              " 'transit_mixer': 58,\n",
              " 'vcb_panel': 59,\n",
              " 'vitrified_tiles': 60,\n",
              " 'vrf_units': 61,\n",
              " 'wall': 12,\n",
              " 'water': 11,\n",
              " 'water_tank': 62,\n",
              " 'wheel_loader': 63,\n",
              " 'window': 13,\n",
              " 'wood_primer': 64}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "t6gFcHBirhdd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions for Dataset Creation\n",
        "\n",
        "- Resize Binary Mask\n",
        "- Close the Contour\n",
        "- Binary Mask to Polygon\n",
        "- Create Image Data and JSON\n",
        "- Create Annotation Data"
      ],
      "metadata": {
        "id": "fmcuC6mdSx0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import os\n",
        "import re\n",
        "import datetime\n",
        "import numpy as np\n",
        "from itertools import groupby\n",
        "from skimage import measure\n",
        "from PIL import Image\n",
        "from pycocotools import mask\n",
        "\n",
        "convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
        "natrual_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ]\n",
        "\n",
        "def resize_binary_mask(array, new_size):\n",
        "    image = Image.fromarray(array.astype(np.uint8)*255)\n",
        "    image = image.resize(new_size)\n",
        "    return np.asarray(image).astype(np.bool_)\n",
        "\n",
        "def close_contour(contour):\n",
        "    if not np.array_equal(contour[0], contour[-1]):\n",
        "        contour = np.vstack((contour, contour[0]))\n",
        "    return contour\n",
        "\n",
        "def binary_mask_to_rle(binary_mask):\n",
        "    rle = {'counts': [], 'size': list(binary_mask.shape)}\n",
        "    counts = rle.get('counts')\n",
        "    for i, (value, elements) in enumerate(groupby(binary_mask.ravel(order='F'))):\n",
        "        if i == 0 and value == 1:\n",
        "                counts.append(0)\n",
        "        counts.append(len(list(elements)))\n",
        "\n",
        "    return rle\n",
        "\n",
        "def binary_mask_to_polygon(binary_mask, tolerance=0):\n",
        "    \"\"\"Converts a binary mask to COCO polygon representation\n",
        "\n",
        "    Args:\n",
        "        binary_mask: a 2D binary numpy array where '1's represent the object\n",
        "        tolerance: Maximum distance from original points of polygon to approximated\n",
        "            polygonal chain. If tolerance is 0, the original coordinate array is returned.\n",
        "\n",
        "    \"\"\"\n",
        "    polygons = []\n",
        "    # pad mask to close contours of shapes which start and end at an edge\n",
        "    padded_binary_mask = np.pad(binary_mask, pad_width=1, mode='constant', constant_values=0)\n",
        "    contours = measure.find_contours(padded_binary_mask, 0.5)\n",
        "    contours = np.subtract(contours, 1)\n",
        "    for contour in contours:\n",
        "        contour = close_contour(contour)\n",
        "        contour = measure.approximate_polygon(contour, tolerance)\n",
        "        if len(contour) < 3:\n",
        "            continue\n",
        "        contour = np.flip(contour, axis=1)\n",
        "        segmentation = contour.ravel().tolist()\n",
        "        # after padding and subtracting 1 we may get -0.5 points in our segmentation \n",
        "        segmentation = [0 if i < 0 else i for i in segmentation]\n",
        "        polygons.append(segmentation)\n",
        "\n",
        "    return polygons\n",
        "\n",
        "def create_image_info(image_id, file_name, image_size, \n",
        "                      date_captured=datetime.datetime.utcnow().isoformat(' '),\n",
        "                      license_id=1, coco_url=\"\", flickr_url=\"\"):\n",
        "\n",
        "    image_info = {\n",
        "            \"id\": image_id,\n",
        "            \"file_name\": file_name,\n",
        "            \"width\": image_size[0],\n",
        "            \"height\": image_size[1],\n",
        "            \"date_captured\": date_captured,\n",
        "            \"license\": license_id,\n",
        "            \"coco_url\": coco_url,\n",
        "            \"flickr_url\": flickr_url\n",
        "    }\n",
        "\n",
        "    return image_info\n",
        "\n",
        "def create_annotation_info(annotation_id, image_id, category_info, binary_mask, \n",
        "                           image_size=None, tolerance=2, bounding_box=None):\n",
        "\n",
        "    if image_size is not None:\n",
        "        binary_mask = resize_binary_mask(binary_mask, image_size)\n",
        "\n",
        "    binary_mask_encoded = mask.encode(np.asfortranarray(binary_mask.astype(np.uint8)))\n",
        "\n",
        "    area = mask.area(binary_mask_encoded)\n",
        "    if area < 1:\n",
        "        return None\n",
        "\n",
        "    if bounding_box is None:\n",
        "        bounding_box = mask.toBbox(binary_mask_encoded)\n",
        "\n",
        "    if category_info[\"is_crowd\"]:\n",
        "        is_crowd = 1\n",
        "        segmentation = binary_mask_to_rle(binary_mask)\n",
        "    else :\n",
        "        is_crowd = 0\n",
        "        segmentation = binary_mask_to_polygon(binary_mask, tolerance)\n",
        "        if not segmentation:\n",
        "            return None\n",
        "\n",
        "    annotation_info = {\n",
        "        \"id\": annotation_id,\n",
        "        \"image_id\": image_id,\n",
        "        \"category_id\": category_info[\"id\"],\n",
        "        \"iscrowd\": is_crowd,\n",
        "        \"area\": area.tolist(),\n",
        "        \"bbox\": bounding_box.tolist(),\n",
        "        \"segmentation\": segmentation,\n",
        "        \"width\": binary_mask.shape[1],\n",
        "        \"height\": binary_mask.shape[0],\n",
        "    } \n",
        "\n",
        "    return annotation_info"
      ],
      "metadata": {
        "id": "uuykHZsR6f9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Super Impose Function\n",
        "\n",
        "- To overlay our construction materials on top of COCO output predicted by Facebook DETR"
      ],
      "metadata": {
        "id": "Tyvm8lOtTD5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from math import floor\n",
        "\n",
        "def superimpose_thing(image_size, annotations):\n",
        "    height, width = image_size\n",
        "\n",
        "    # create a single channel black image\n",
        "    superimposed_image = np.zeros((height, width))\n",
        "\n",
        "    polygons_list = []\n",
        "    # Add the polygon segmentation\n",
        "    for segmentation_points in annotation['segmentation']:\n",
        "        segmentation_points = np.multiply(segmentation_points, 1).astype(int)\n",
        "        polygons_list.append(segmentation_points)\n",
        "\n",
        "    # convert segmentation points to contour\n",
        "    for x in polygons_list:\n",
        "        end = []\n",
        "        if len(x) % 2 != 0:\n",
        "            print(x)\n",
        "        for l in range(0, len(x), 2):\n",
        "            coords = [floor(x[l]), floor(x[l + 1])]\n",
        "            end.append(coords)\n",
        "        contours = np.array(end)\n",
        "        if end == []:\n",
        "            continue\n",
        "\n",
        "        # plot and fill the contour\n",
        "        cv2.fillPoly(superimposed_image, pts=[contours], color=(1, 1, 1))\n",
        "  \n",
        "    return superimposed_image"
      ],
      "metadata": {
        "id": "8P7MDBIC6hfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pycocotools import mask\n",
        "from skimage import measure\n",
        "\n",
        "def make_contours_closed(contour):\n",
        "    if not np.array_equal(contour[0], contour[-1]):\n",
        "        contour = np.vstack((contour, contour[0]))\n",
        "    return contour\n",
        "    \n",
        "\n",
        "def binary_mask_to_polygon(binary_mask, tolerance=0):\n",
        "    \"\"\"Converts a binary mask to COCO polygon representation\n",
        "\n",
        "    Args:\n",
        "        binary_mask: a 2D binary numpy array where '1's represent the object\n",
        "        tolerance: Maximum distance from original points of polygon to approximated\n",
        "            polygonal chain. If tolerance is 0, the original coordinate array is returned.\n",
        "\n",
        "    \"\"\"\n",
        "    polygons = []\n",
        "    # pad mask to close contours of shapes which start and end at an edge\n",
        "    padded_binary_mask = np.pad(binary_mask, pad_width=1, mode='constant', constant_values=0)\n",
        "    contours = measure.find_contours(padded_binary_mask, 0.5)\n",
        "    contours = np.subtract(contours, 1)\n",
        "    for contour in contours:\n",
        "        contour = make_contours_closed(contour)\n",
        "        contour = measure.approximate_polygon(contour, tolerance)\n",
        "        if len(contour) < 3:\n",
        "            continue\n",
        "        contour = np.flip(contour, axis=1)\n",
        "        segmentation = contour.ravel().tolist()\n",
        "        # after padding and subtracting 1 we may get -0.5 points in our segmentation \n",
        "        segmentation = [0 if i < 0 else i for i in segmentation]\n",
        "        polygons.append(segmentation)\n",
        "\n",
        "    return polygons\n",
        "\n",
        "def convert(o):\n",
        "    if isinstance(o, np.generic): return o.item()  \n",
        "    raise TypeError\n",
        "\n",
        "\n",
        "\n",
        "def get_annotation_info(binary_mask, image_size, image_id, class_id, segmentation_id, iscrowd):\n",
        "\n",
        "    category_info = {'id': class_id, 'is_crowd': iscrowd}\n",
        "\n",
        "    tolerance = 2\n",
        "    bounding_box = None    \n",
        "\n",
        "    if image_size is not None:\n",
        "        binary_mask = resize_binary_mask(binary_mask, image_size)\n",
        "\n",
        "    binary_mask_encoded = mask.encode(np.asfortranarray(binary_mask.astype(np.uint8)))\n",
        "\n",
        "    area = mask.area(binary_mask_encoded)\n",
        "    if area < 1:\n",
        "        return None\n",
        "\n",
        "    if bounding_box is None:\n",
        "        bounding_box = mask.toBbox(binary_mask_encoded)\n",
        "\n",
        "    if category_info[\"is_crowd\"]:\n",
        "        is_crowd = 1\n",
        "        segmentation = binary_mask_to_rle(binary_mask)\n",
        "    else :\n",
        "        is_crowd = 0\n",
        "        segmentation = binary_mask_to_polygon(binary_mask, tolerance)\n",
        "        if not segmentation:\n",
        "            return None\n",
        "\n",
        "    annotation_info = {\n",
        "        \"id\": annotation_id,\n",
        "        \"image_id\": image_id,\n",
        "        \"category_id\": category_info[\"id\"],\n",
        "        \"iscrowd\": is_crowd,\n",
        "        \"area\": area.tolist(),\n",
        "        \"bbox\": bounding_box.tolist(),\n",
        "        \"segmentation\": segmentation,\n",
        "        \"width\": binary_mask.shape[1],\n",
        "        \"height\": binary_mask.shape[0],\n",
        "    } \n",
        "\n",
        "\n",
        "    return annotation_info"
      ],
      "metadata": {
        "id": "c5BPhDNb6psj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import and Load FACEBOOK DETR Pre-Trained Model"
      ],
      "metadata": {
        "id": "skOWfDSKTOdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as T\n",
        "\n",
        "# standard PyTorch mean-std input image normalization\n",
        "transform = T.Compose([\n",
        "    T.Resize(800),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load detr model\n",
        "model, postprocessor = torch.hub.load('detr', 'detr_resnet101_panoptic', source='local', pretrained=True, return_postprocessor=True, num_classes=250)\n",
        "# Convert to eval mode\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"Model Loaded\")\n",
        "\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOU-lAlttlqw",
        "outputId": "19dd8413-4044-4c24-f6df-40ce0fb5a02b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Loaded\n",
            "/content/drive/MyDrive/Panoptic Segmentation using DETR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mask Image Creation and JSON Creation\n",
        "1. For all categories and images\n",
        "2. Pass Image to Facebbok model\n",
        "3. Gather predictions\n",
        "4. COnvert predictions to our mapping\n",
        "5. Overlay construction materials on top of COCO classes\n",
        "6. Adjust Mask images and mappings accordingly\n",
        "7. Save Mask Image and JSON\n"
      ],
      "metadata": {
        "id": "1l4iuemoTUwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bjSIKf0ttr6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "import io\n",
        "import panopticapi\n",
        "from panopticapi.utils import id2rgb, rgb2id\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import numpy\n",
        "torch.set_grad_enabled(False)\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "image_id = 1\n",
        "annotation_id = 1\n",
        "segment_id = 1\n",
        "\n",
        "detection_coco = {\n",
        "    \"categories\": categories_for_pp_project,\n",
        "    \"annotations\": [],\n",
        "    \"images\": []\n",
        "}\n",
        "\n",
        "panoptic_coco = {\n",
        "    \"categories\": categories_for_pp_project,\n",
        "    \"annotations\": [],\n",
        "    \"images\": []\n",
        "}\n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/Panoptic Segmentation using DETR/Dataset\")\n",
        "cats = os.listdir()\n",
        "\n",
        "os.makedirs(\"/content/drive/MyDrive/Panoptic Segmentation using DETR/data/train\", exist_ok=True)\n",
        "os.makedirs(\"/content/drive/MyDrive/Panoptic Segmentation using DETR/data/panoptic_train\", exist_ok=True)\n",
        "\n",
        "final_images_path = \"/content/drive/MyDrive/Panoptic Segmentation using DETR/data/\"\n",
        "\n",
        "\n",
        "# run through all folders in dataset\n",
        "for cat in cats:\n",
        "    if os.path.isfile(cat):\n",
        "      continue\n",
        "\n",
        "    # get category name\n",
        "    category_name = cat\n",
        "    print(\"Category:\", category_name)\n",
        "    with open(os.path.join(cat, \"coco.json\"), \"r\") as json_file:\n",
        "        category_json = json.load(json_file)\n",
        "        \n",
        "    images_path = os.path.join(cat, 'images')\n",
        "        \n",
        "    temporary_annotations = {}\n",
        "    \n",
        "    # Run over all images\n",
        "    for image_info in category_json[\"images\"]:\n",
        "        image_info['annotations'] = []\n",
        "        temporary_annotations[image_info['id']] = image_info\n",
        "        \n",
        "    for annnotation_info in category_json[\"annotations\"]:\n",
        "        temporary_annotations[annnotation_info['image_id']][\"annotations\"].append(annnotation_info)\n",
        "        \n",
        "    for i, image_item in temporary_annotations.items():\n",
        "\n",
        "        output_file_name = category_name + \"_\" + str(image_id) + \".jpg\"\n",
        "        output_file_path = os.path.join(final_images_path, \"train\", output_file_name)\n",
        "        \n",
        "        output_mask_name = category_name + \"_\" + str(image_id) + \".png\"\n",
        "        output_mask_path = os.path.join(final_images_path, \"panoptic_train\", output_mask_name)\n",
        "        \n",
        "        try:\n",
        "\n",
        "            # Read the image and get shape of image\n",
        "            original_image = Image.open(os.path.join(images_path, image_item['file_name'])).convert('RGB')\n",
        "\n",
        "            try:\n",
        "                h, w, c = np.array(original_image).shape\n",
        "            except:\n",
        "                h, w = np.array(original_image).shape\n",
        "                c = 1\n",
        "\n",
        "            # if no of channels != 3, open the image and convert it to 3 channel - RGB\n",
        "            if c == 4 or c == 1:\n",
        "                original_image = original_image.convert('RGB')\n",
        "                h, w, c = np.array(original_image).shape\n",
        "\n",
        "            duplicate_image = original_image.copy()\n",
        "\n",
        "            # Apply transform and convert image to batch\n",
        "            # mean-std normalize the input image (batch-size: 1)\n",
        "            img = transform(duplicate_image).unsqueeze(0).to(device)  # [h, w, c] -> [1, c, ht, wt]\n",
        "\n",
        "            # Generate output for image\n",
        "            out = model(img)\n",
        "\n",
        "            # Generate score\n",
        "            # compute the scores, excluding the \"no-object\" class (the last one)\n",
        "            scores = out[\"pred_logits\"].softmax(-1)[..., :-1].max(-1)[0]\n",
        "\n",
        "            # threshold the confidence\n",
        "            keep = scores > 0.85\n",
        "\n",
        "            # Keep only ones above threshold\n",
        "            pred_logits, pred_boxes = out[\"pred_logits\"][keep][:, :len(\n",
        "                coco_category_names) - 1], out[\"pred_boxes\"][keep]\n",
        "\n",
        "            # the post-processor expects as input the target size of the predictions (which we set here to the image size)\n",
        "            result = postprocessor(out, torch.as_tensor(img.shape[-2:]).unsqueeze(0))[0]\n",
        "\n",
        "            # The segmentation is stored in a special-format png\n",
        "            panoptic_seg = Image.open(io.BytesIO(result['png_string'])).resize((w, h), Image.NEAREST)\n",
        "            # (wp, hp) = panoptic_seg.size\n",
        "            panoptic_seg = np.array(panoptic_seg, dtype=np.uint8).copy()\n",
        "\n",
        "            # We retrieve the ids corresponding to each mask\n",
        "            panoptic_seg_id = rgb2id(panoptic_seg)\n",
        "            \n",
        "            # get unique prediction ids\n",
        "            unique_category_id = []\n",
        "            for i, segment in enumerate(result['segments_info']):\n",
        "                result['segments_info'][i][\"category_id\"] = old_to_new_category_mapping[result['segments_info'][i][\"category_id\"]]\n",
        "                if result['segments_info'][i][\"category_id\"] not in unique_category_id:\n",
        "                    unique_category_id.append(result['segments_info'][i][\"category_id\"])\n",
        "            \n",
        "            # Sort array\n",
        "            unique_category_id.sort()\n",
        "            \n",
        "            unique_category_id_to_id =  {category_id: i for i, category_id in enumerate(unique_category_id)}\n",
        "            unique_id_to_category_id =  {i: category_id for category_id, i in unique_category_id_to_id.items()}\n",
        "            \n",
        "            for i, segment in enumerate(result['segments_info']):\n",
        "                result['segments_info'][i][\"new_id\"] = unique_category_id_to_id[result['segments_info'][i][\"category_id\"]]\n",
        "            \n",
        "            # Update original panoptic_seg_id array with new ids as the new segmentation combines different categories.\n",
        "            custom_panoptic_seg_id = np.zeros((panoptic_seg_id.shape[0], panoptic_seg_id.shape[1]), dtype=np.uint8)\n",
        "            \n",
        "            # Update this custom panoptic seg matrix\n",
        "            for i, segment in enumerate(result['segments_info']):\n",
        "                custom_panoptic_seg_id[result['segments_info'][i]['id'] == panoptic_seg_id] = result['segments_info'][i]['new_id']\n",
        "            \n",
        "            custom_panoptic_segments_info = []\n",
        "            for category_id in unique_category_id:\n",
        "                custom_panoptic_segments_info.append({\n",
        "                    'segment_id': unique_category_id_to_id[category_id], \n",
        "                    'category_id': category_id,\n",
        "                    'bbox': [],\n",
        "                    'area': 0,\n",
        "                    'iscrowd': 0,\n",
        "                    'isthing': 0\n",
        "                })\n",
        "\n",
        "            # annotations of our construction things\n",
        "            original_mask = image_item['annotations']\n",
        "            \n",
        "            to_append_annotations = []\n",
        "            \n",
        "            # Overlay things mask one at a time\n",
        "            for annotation in original_mask:\n",
        "                # overlay mask of construction things on top of detr output\n",
        "                omask_image_id = superimpose_thing((h, w), annotation)\n",
        "                custom_panoptic_seg_id[omask_image_id.astype(np.bool_)] = custom_panoptic_seg_id.max() + 1\n",
        "                custom_panoptic_segments_info.append({\n",
        "                    'segment_id': custom_panoptic_seg_id.max(), \n",
        "                    'category_id': category_to_id[category_name], \n",
        "                    'bbox': annotation['bbox'],\n",
        "                    'area': annotation['area'],\n",
        "                    'iscrowd': 0,\n",
        "                    'isthing': 1\n",
        "                })\n",
        "\n",
        "                # append annotation of construction things in json file\n",
        "                annotation[\"category_id\"] = category_to_id[category_name]\n",
        "                annotation[\"image_id\"] = image_id\n",
        "                to_append_annotations.append(annotation)\n",
        "            \n",
        "            # Convert to binary segment\n",
        "            binary_masks = np.zeros((\n",
        "                custom_panoptic_seg_id.max() + 1,\n",
        "                custom_panoptic_seg_id.shape[0],\n",
        "                custom_panoptic_seg_id.shape[1]),\n",
        "                dtype=np.uint8\n",
        "            )\n",
        "\n",
        "            # for each binary mask, detect contours and create annotation for those contours\n",
        "            if len(unique_category_id):\n",
        "                # Skip the onse which are added by us\n",
        "                for category_id in unique_category_id:\n",
        "                    binary_masks[unique_category_id_to_id[category_id], :, :] = custom_panoptic_seg_id == unique_category_id_to_id[category_id]\n",
        "                    annotation_info = get_annotation_info(binary_masks[unique_category_id_to_id[category_id]], None, image_id, category_id, unique_category_id_to_id[category_id], 0)\n",
        "                    if annotation_info is not None:\n",
        "                        annotation_info[\"image_id\"] = image_id\n",
        "                        annotation_info[\"category_id\"] = category_id\n",
        "                        to_append_annotations.append(annotation_info)\n",
        "                        \n",
        "                        custom_panoptic_segments_info[unique_category_id_to_id[category_id]]['bbox'] = annotation_info['bbox']\n",
        "                        custom_panoptic_segments_info[unique_category_id_to_id[category_id]]['area'] = annotation_info['area']\n",
        "            else:\n",
        "                pass\n",
        "            \n",
        "            # save image to new path as .jpg\n",
        "            original_image.save(output_file_path)\n",
        "            \n",
        "            # save panoptic image\n",
        "            Image.fromarray(id2rgb(custom_panoptic_seg_id), 'RGB').save(output_mask_path)\n",
        "\n",
        "            # create image_info object and append it to original list\n",
        "            image_info = {\n",
        "                \"id\": image_id,\n",
        "                \"file_name\": output_file_name,\n",
        "                \"width\": original_image.size[0],\n",
        "                \"height\": original_image.size[1]\n",
        "                }\n",
        "            \n",
        "            detection_coco[\"images\"].append(image_info)\n",
        "            panoptic_coco[\"images\"].append(image_info)\n",
        "\n",
        "            for annotation in to_append_annotations:\n",
        "                annotation[\"id\"] = annotation_id\n",
        "                detection_coco[\"annotations\"].append(annotation)\n",
        "                annotation_id += 1\n",
        "                \n",
        "            for segment_info in custom_panoptic_segments_info:\n",
        "                segment_info[\"id\"] = segment_id\n",
        "                segment_id += 1\n",
        "                \n",
        "            panoptic_coco[\"annotations\"].append({\n",
        "                \"segments_info\": custom_panoptic_segments_info,\n",
        "                \"file_name\": output_mask_name,\n",
        "                \"image_id\": image_id\n",
        "            })\n",
        "\n",
        "            # increment the image_count\n",
        "            image_id += 1\n",
        "    \n",
        "        except Exception as e:\n",
        "            # print(\"Error ******** :\", os.path.join(images_path, image_item['file_name']))\n",
        "            continue    \n",
        "\n",
        "    # open the final json, and commit changes in that file\n",
        "    with open(os.path.join(final_images_path, \"train.json\"), 'w') as output_json_file:\n",
        "        json.dump(detection_coco, output_json_file)\n",
        "        \n",
        "    with open(os.path.join(final_images_path, \"panoptic_train.json\"), 'w') as output_json_file:\n",
        "        json.dump(panoptic_coco, output_json_file, default=convert)\n",
        "        \n",
        "    print(image_id, annotation_id, segment_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhGN2YuOtuOJ",
        "outputId": "1539dc28-987b-4605-aeed-7e6f267cd62d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Category: distribution_transformer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "detr/models/position_encoding.py:41: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "412 1817 1820\n",
            "Category: aac_blocks\n",
            "675 2853 2857\n",
            "Category: ahus\n",
            "808 3217 3227\n",
            "Category: control_panel\n",
            "1053 3905 3928\n",
            "Category: cu_piping\n",
            "1547 5254 5284\n",
            "Category: concrete_pump_(50_)\n",
            "1547 5254 5284\n",
            "Category: concrete_mixer_machine\n",
            "1597 5430 5461\n",
            "Category: adhesives\n",
            "1697 5638 5669\n",
            "Category: aluminium_frames_for_false_ceiling\n",
            "1747 5780 5812\n",
            "Category: chiller\n",
            "1792 5930 5966\n",
            "Category: dump_truck___tipper_truck\n",
            "1792 5930 5966\n",
            "Category: glass_wool\n",
            "1862 6238 6274\n",
            "Category: hollow_concrete_blocks\n",
            "1912 6398 6435\n",
            "Category: hoist\n",
            "2447 8392 8451\n",
            "Category: emulsion_paint\n",
            "2479 8471 8532\n",
            "Category: fire_extinguishers\n",
            "2707 9097 9161\n",
            "Category: enamel_paint\n",
            "2758 9217 9283\n",
            "Category: grader\n",
            "3058 10620 10692\n",
            "Category: fire_buckets\n",
            "3516 12073 12152\n",
            "Category: fine_aggregate\n",
            "4016 13053 13134\n",
            "Category: junction_box\n",
            "4068 13193 13275\n",
            "Category: marble\n",
            "4118 13378 13463\n",
            "Category: metal_primer\n",
            "4177 13499 13584\n",
            "Category: lime\n",
            "4689 15901 15989\n",
            "Category: rcc_hume_pipes\n",
            "4929 16613 16704\n",
            "Category: hot_mix_plant\n",
            "5029 17087 17183\n",
            "Category: hydra_crane\n",
            "5129 17502 17600\n",
            "Category: pipe_fittings\n",
            "5183 17618 17716\n",
            "Category: refrigerant_gas\n",
            "5281 17947 18046\n",
            "Category: interlocked_switched_socket\n",
            "5381 18103 18205\n",
            "Category: skid_steer_loader_(bobcat)\n",
            "5384 18115 18217\n",
            "Category: split_units\n",
            "5922 19782 19896\n",
            "Category: rmu_units\n",
            "6022 20027 20142\n",
            "Category: smoke_detectors\n",
            "6072 20121 20236\n",
            "Category: switch_boards_and_switches\n",
            "6572 21157 21305\n",
            "Category: sanitary_fixtures\n",
            "7093 23827 23977\n",
            "Category: rmc_batching_plant\n",
            "7152 24127 24282\n",
            "Category: structural_steel_-_channel\n",
            "7153 24128 24283\n",
            "Category: texture_paint\n",
            "7205 24231 24388\n",
            "Category: river_sand\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pending\n",
        "\n",
        "- JSON Creation\n",
        "- Split 80 : 20\n",
        "- Preprocessing Mask Images and JSON\n"
      ],
      "metadata": {
        "id": "Nb2N-vgstAKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/Panoptic Segmentation using DETR/data\")\n",
        "\n",
        "f = open(\"train.json\")\n",
        "train_json = json.load(f)\n",
        "f.close()\n",
        "\n",
        "f = open(\"panoptic_train.json\")\n",
        "panoptic_train_json = json.load(f)\n",
        "f.close()\n",
        "\n",
        "train_images = train_json[\"images\"]\n",
        "train_annotations = train_json[\"annotations\"]\n",
        "\n",
        "val_images = []\n",
        "val_annotations = []\n",
        "\n",
        "panoptic_train_images = panoptic_train_json[\"images\"]\n",
        "panoptic_train_annotations = panoptic_train_json[\"annotations\"]\n",
        "\n",
        "panoptic_val_images = []\n",
        "panoptic_val_annotations = []\n",
        "\n",
        "\n",
        "print(len(train_images), len(val_images), len(train_annotations), len(val_annotations), len(panoptic_train_images), len(panoptic_train_annotations), len(panoptic_val_images), len(panoptic_val_annotations))\n",
        "\n",
        "count = 0\n",
        "while True:\n",
        "    if count >= 2000:\n",
        "        break\n",
        "    random.shuffle(train_images)\n",
        "    to_del = train_images[0]\n",
        "    found = False\n",
        "    for x in panoptic_train_images:\n",
        "        if x[\"id\"] == to_del[\"id\"]:\n",
        "            panoptic_val_images.append(x)\n",
        "            panoptic_train_images.remove(x)\n",
        "            found = True\n",
        "    if not found:\n",
        "        print(to_del)\n",
        "    if found:\n",
        "        val_images.append(train_images[0])\n",
        "        for x in panoptic_train_annotations:\n",
        "            if x[\"image_id\"] == to_del[\"id\"]:\n",
        "                panoptic_val_annotations.append(x)\n",
        "                panoptic_train_annotations.remove(x)\n",
        "        for x in train_annotations:\n",
        "            if x[\"image_id\"] == to_del[\"id\"]:\n",
        "                val_annotations.append(x)\n",
        "                train_annotations.remove(x)\n",
        "        train_images.remove(to_del)\n",
        "        count += 1\n",
        "\n",
        "\n",
        "print(len(train_images), len(val_images), len(train_annotations), len(val_annotations), len(panoptic_train_images), len(panoptic_train_annotations), len(panoptic_val_images), len(panoptic_val_annotations))\n",
        "\n",
        "\n",
        "train_json[\"images\"] = train_images\n",
        "train_json[\"annotations\"] = train_annotations\n",
        "\n",
        "\n",
        "val_json[\"images\"] = val_images\n",
        "val_json[\"annotations\"] = val_annotations\n",
        "\n",
        "panoptic_train_json[\"images\"] = panoptic_train_images\n",
        "panoptic_train_json[\"annotations\"] = panoptic_train_annotations\n",
        "\n",
        "panoptic_val_json[\"images\"] = panoptic_val_images\n",
        "panoptic_val_json[\"annotations\"] = panoptic_val_annotations\n",
        "\n",
        "f = open(\"train.json\", \"w\")\n",
        "json.dump(train_json, f)\n",
        "f.close()\n",
        "\n",
        "f = open(\"val.json\", \"w\")\n",
        "json.dump(val_json, f)\n",
        "f.close()\n",
        "\n",
        "f = open(\"panoptic_train.json\", \"w\")\n",
        "json.dump(panoptic_train_json, f)\n",
        "f.close()\n",
        "\n",
        "f = open(\"panoptic_val.json\", \"w\")\n",
        "json.dump(panoptic_val_json, f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "NNdHh9QstBnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/Panoptic Segmentation using DETR/data\")\n",
        "\n",
        "files = [\"train.json\", \"val.json\"]\n",
        "\n",
        "for file in files:\n",
        "    f = open(file)\n",
        "    j = json.load(f)\n",
        "\n",
        "    for x in j[\"annotations\"]:\n",
        "        if x[\"segmentation\"] == []:\n",
        "            print(x)\n",
        "            j[\"annotations\"].remove(x)\n",
        "\n",
        "    f = open(file, \"w\")\n",
        "    json.dump(j, f)\n",
        "    f.close()\n",
        "\n",
        "\n",
        "\n",
        "files = [\"panoptic_train.json\", \"panoptic_val.json\"]\n",
        "\n",
        "for file in files:\n",
        "    f = open(file)\n",
        "    j = json.load(f)\n",
        "\n",
        "    for x in j[\"annotations\"]:\n",
        "        for y in x[\"segments_info\"]:\n",
        "            if len(y[\"bbox\"]) != 4:\n",
        "                x[\"segments_info\"].remove(y)\n",
        "\n",
        "    f = open(file, \"w\")\n",
        "    json.dump(j, f)\n",
        "    f.close()\n"
      ],
      "metadata": {
        "id": "6Xy5_Z6QITEg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}