{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dataset Creation",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "aeb29033d1ae48e6a8ff12d926a26e9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0eec2c8647af42dd86dda3754fbfa44f",
              "IPY_MODEL_cb69729e5e5f4c5db403f06b98f5cd07",
              "IPY_MODEL_04f72e2acaa9428baee882719a032ece"
            ],
            "layout": "IPY_MODEL_a981d9fe0811408e91520ec08ff07084"
          }
        },
        "0eec2c8647af42dd86dda3754fbfa44f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b786291b3b142bb8f4edc823a6b7cbb",
            "placeholder": "​",
            "style": "IPY_MODEL_b5f0c7e6ce234c28948d003f71dcafc2",
            "value": "100%"
          }
        },
        "cb69729e5e5f4c5db403f06b98f5cd07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d6a343d22c2433cb9f3ffaf958b4538",
            "max": 178793939,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6e35a00e727849d8ac56c7a88f99c49e",
            "value": 178793939
          }
        },
        "04f72e2acaa9428baee882719a032ece": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dddcaa9ea4274953b8a874bc3258cbad",
            "placeholder": "​",
            "style": "IPY_MODEL_3f93a63605ce407d9823877cef09008e",
            "value": " 171M/171M [00:00&lt;00:00, 278MB/s]"
          }
        },
        "a981d9fe0811408e91520ec08ff07084": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b786291b3b142bb8f4edc823a6b7cbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5f0c7e6ce234c28948d003f71dcafc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d6a343d22c2433cb9f3ffaf958b4538": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e35a00e727849d8ac56c7a88f99c49e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dddcaa9ea4274953b8a874bc3258cbad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f93a63605ce407d9823877cef09008e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41aab71fb0134c659e4d415fe1e0ef7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a072b4e6ba4b40d6a9c59bc2ccc34963",
              "IPY_MODEL_dbac77706d7c411dadbabe8b529fa09a",
              "IPY_MODEL_8bd5bc89239e42cba5a45c56a0fc4834"
            ],
            "layout": "IPY_MODEL_1f1bf0f27c5f4ea48fe6bff5f13f8b82"
          }
        },
        "a072b4e6ba4b40d6a9c59bc2ccc34963": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf27be1e62d94a8db42660d7f47ad40a",
            "placeholder": "​",
            "style": "IPY_MODEL_4cf3900d712149868884dea8b4536782",
            "value": "100%"
          }
        },
        "dbac77706d7c411dadbabe8b529fa09a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_675e00ea08cd49d1a49f25b59772598c",
            "max": 248356138,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_00244c4b97a64066af4a1d17dd85b17f",
            "value": 248356138
          }
        },
        "8bd5bc89239e42cba5a45c56a0fc4834": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfa08b26a24640baa3ce906202e5dfb4",
            "placeholder": "​",
            "style": "IPY_MODEL_a93a9c9ac918429a8d6d26ef01c1f92c",
            "value": " 237M/237M [00:11&lt;00:00, 23.2MB/s]"
          }
        },
        "1f1bf0f27c5f4ea48fe6bff5f13f8b82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf27be1e62d94a8db42660d7f47ad40a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cf3900d712149868884dea8b4536782": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "675e00ea08cd49d1a49f25b59772598c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00244c4b97a64066af4a1d17dd85b17f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cfa08b26a24640baa3ce906202e5dfb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a93a9c9ac918429a8d6d26ef01c1f92c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amanjain487/panoptic-segmentation-using-DETR/blob/main/Dataset_Creation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Dataset for Construction Material + COCO Classes\n",
        "\n",
        "## Following Operations are performed using this colab file\n",
        "- Given, construction dataset - things in our case\n",
        "- use pretrained DETR panoptic model to predict stuff and things for all given images\n",
        "- Add all stuff predictions as ground truth for our dataset and all things predictions as misc stuff class in ground truth\n",
        "- Test - Train split "
      ],
      "metadata": {
        "id": "6_emsoUYPHlW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Requirements and Prepare the Notebook"
      ],
      "metadata": {
        "id": "XDPeEwMJRK7d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STC41cG3O35Z",
        "outputId": "fa156b61-0363-464f-fa24-348267dc215e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch 1.10.0+cu111 _CudaDeviceProperties(name='Tesla V100-SXM2-16GB', major=7, minor=0, total_memory=16160MB, multi_processor_count=80)\n",
            "Collecting git+https://github.com/cocodataset/panopticapi.git\n",
            "  Cloning https://github.com/cocodataset/panopticapi.git to /tmp/pip-req-build-30_ufvg2\n",
            "  Running command git clone -q https://github.com/cocodataset/panopticapi.git /tmp/pip-req-build-30_ufvg2\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from panopticapi==0.1) (1.21.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from panopticapi==0.1) (7.1.2)\n",
            "Building wheels for collected packages: panopticapi\n",
            "  Building wheel for panopticapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for panopticapi: filename=panopticapi-0.1-py3-none-any.whl size=8306 sha256=8e37652a1de922420d04be0195a1af08dea9be8f143ad69c7a515b15da40d6c0\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-le8l68e6/wheels/ad/89/b8/b66cce9246af3d71d65d72c85ab993fd28e7578e1b0ed197f1\n",
            "Successfully built panopticapi\n",
            "Installing collected packages: panopticapi\n",
            "Successfully installed panopticapi-0.1\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import glob\n",
        "import torch\n",
        "import os\n",
        "\n",
        "from IPython.display import Image, clear_output \n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('PyTorch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n",
        "\n",
        "!pip install git+https://github.com/cocodataset/panopticapi.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Drive \n",
        "\n",
        "We need drive access for the following things\n",
        "- Our dataset is stored in drive - to access the dataset\n",
        "- To extract original dataset directly in drive - and use them for groud truth creation\n",
        "- Create ground truths directly in drive"
      ],
      "metadata": {
        "id": "aJdFx4jtP_sA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9-F3aDVO5IE",
        "outputId": "abdbb4ab-4d48-4cac-b885-652a00fe8a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unzip the given original dataset\n",
        "\n",
        "- The dataset is uploaded in zip format\n",
        "- Unzip the dataset in drive itself\n",
        "- Once, unzipped, delete the zip file, to manage drive's limited space"
      ],
      "metadata": {
        "id": "3J69eUbVQJur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.chdir(\"/content/drive/MyDrive/Panoptic Segmentation using DETR/Original Dataset\")\n",
        "\n",
        "# import zipfile\n",
        "# !unzip construction_materials_dataset.zip \n",
        "# os.remove(\"construction_materials_dataset.zip\")"
      ],
      "metadata": {
        "id": "LDZRdzkBP-qK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone official DETR Repo to Drive\n",
        "- We will be passing our dataset images through official pre-trained DETR model by Facebook for getting predictions of COCO Classes\n",
        "- The output of DETR model will have lot of predictions ranging from 0% confidence to 100% confidence\n",
        "- All the predictions with confidence greater than 85% will become our ground truth for COCO classes which will be later combined with `construction materials` annotations which will become our final Dataset. "
      ],
      "metadata": {
        "id": "hlQO1MBCosv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/Panoptic Segmentation using DETR/\")\n",
        "!git clone https://github.com/facebookresearch/detr.git\n",
        "sys.path.append(os.path.join(os.getcwd(), \"detr/\"))\n"
      ],
      "metadata": {
        "id": "4S6Rvq07RF8z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2509af5a-7930-4e0b-903c-e2e0d27f1957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'detr' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PVbYHuQ0pSJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define COCO Classes\n",
        "\n",
        "- Define exisitng COCO classes which will be predicted by Facebook DETR\n",
        "- Define COCO classes which we will be using\n",
        "- Establish mapping from esciting to new ones"
      ],
      "metadata": {
        "id": "mx-DnLnbqWXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "existing_coco_categories = [\n",
        "    {\"color\": [220, 20, 60], \"isthing\": 1, \"id\": 1, \"name\": \"person\"},\n",
        "    {\"color\": [119, 11, 32], \"isthing\": 1, \"id\": 2, \"name\": \"bicycle\"},\n",
        "    {\"color\": [0, 0, 142], \"isthing\": 1, \"id\": 3, \"name\": \"car\"},\n",
        "    {\"color\": [0, 0, 230], \"isthing\": 1, \"id\": 4, \"name\": \"motorcycle\"},\n",
        "    {\"color\": [106, 0, 228], \"isthing\": 1, \"id\": 5, \"name\": \"airplane\"},\n",
        "    {\"color\": [0, 60, 100], \"isthing\": 1, \"id\": 6, \"name\": \"bus\"},\n",
        "    {\"color\": [0, 80, 100], \"isthing\": 1, \"id\": 7, \"name\": \"train\"},\n",
        "    {\"color\": [0, 0, 70], \"isthing\": 1, \"id\": 8, \"name\": \"truck\"},\n",
        "    {\"color\": [0, 0, 192], \"isthing\": 1, \"id\": 9, \"name\": \"boat\"},\n",
        "    {\"color\": [250, 170, 30], \"isthing\": 1, \"id\": 10, \"name\": \"traffic light\"},\n",
        "    {\"color\": [100, 170, 30], \"isthing\": 1, \"id\": 11, \"name\": \"fire hydrant\"},\n",
        "    {\"color\": [220, 220, 0], \"isthing\": 1, \"id\": 13, \"name\": \"stop sign\"},\n",
        "    {\"color\": [175, 116, 175], \"isthing\": 1, \"id\": 14, \"name\": \"parking meter\"},\n",
        "    {\"color\": [250, 0, 30], \"isthing\": 1, \"id\": 15, \"name\": \"bench\"},\n",
        "    {\"color\": [165, 42, 42], \"isthing\": 1, \"id\": 16, \"name\": \"bird\"},\n",
        "    {\"color\": [255, 77, 255], \"isthing\": 1, \"id\": 17, \"name\": \"cat\"},\n",
        "    {\"color\": [0, 226, 252], \"isthing\": 1, \"id\": 18, \"name\": \"dog\"},\n",
        "    {\"color\": [182, 182, 255], \"isthing\": 1, \"id\": 19, \"name\": \"horse\"},\n",
        "    {\"color\": [0, 82, 0], \"isthing\": 1, \"id\": 20, \"name\": \"sheep\"},\n",
        "    {\"color\": [120, 166, 157], \"isthing\": 1, \"id\": 21, \"name\": \"cow\"},\n",
        "    {\"color\": [110, 76, 0], \"isthing\": 1, \"id\": 22, \"name\": \"elephant\"},\n",
        "    {\"color\": [174, 57, 255], \"isthing\": 1, \"id\": 23, \"name\": \"bear\"},\n",
        "    {\"color\": [199, 100, 0], \"isthing\": 1, \"id\": 24, \"name\": \"zebra\"},\n",
        "    {\"color\": [72, 0, 118], \"isthing\": 1, \"id\": 25, \"name\": \"giraffe\"},\n",
        "    {\"color\": [255, 179, 240], \"isthing\": 1, \"id\": 27, \"name\": \"backpack\"},\n",
        "    {\"color\": [0, 125, 92], \"isthing\": 1, \"id\": 28, \"name\": \"umbrella\"},\n",
        "    {\"color\": [209, 0, 151], \"isthing\": 1, \"id\": 31, \"name\": \"handbag\"},\n",
        "    {\"color\": [188, 208, 182], \"isthing\": 1, \"id\": 32, \"name\": \"tie\"},\n",
        "    {\"color\": [0, 220, 176], \"isthing\": 1, \"id\": 33, \"name\": \"suitcase\"},\n",
        "    {\"color\": [255, 99, 164], \"isthing\": 1, \"id\": 34, \"name\": \"frisbee\"},\n",
        "    {\"color\": [92, 0, 73], \"isthing\": 1, \"id\": 35, \"name\": \"skis\"},\n",
        "    {\"color\": [133, 129, 255], \"isthing\": 1, \"id\": 36, \"name\": \"snowboard\"},\n",
        "    {\"color\": [78, 180, 255], \"isthing\": 1, \"id\": 37, \"name\": \"sports ball\"},\n",
        "    {\"color\": [0, 228, 0], \"isthing\": 1, \"id\": 38, \"name\": \"kite\"},\n",
        "    {\"color\": [174, 255, 243], \"isthing\": 1, \"id\": 39, \"name\": \"baseball bat\"},\n",
        "    {\"color\": [45, 89, 255], \"isthing\": 1, \"id\": 40, \"name\": \"baseball glove\"},\n",
        "    {\"color\": [134, 134, 103], \"isthing\": 1, \"id\": 41, \"name\": \"skateboard\"},\n",
        "    {\"color\": [145, 148, 174], \"isthing\": 1, \"id\": 42, \"name\": \"surfboard\"},\n",
        "    {\"color\": [255, 208, 186], \"isthing\": 1, \"id\": 43, \"name\": \"tennis racket\"},\n",
        "    {\"color\": [197, 226, 255], \"isthing\": 1, \"id\": 44, \"name\": \"bottle\"},\n",
        "    {\"color\": [171, 134, 1], \"isthing\": 1, \"id\": 46, \"name\": \"wine glass\"},\n",
        "    {\"color\": [109, 63, 54], \"isthing\": 1, \"id\": 47, \"name\": \"cup\"},\n",
        "    {\"color\": [207, 138, 255], \"isthing\": 1, \"id\": 48, \"name\": \"fork\"},\n",
        "    {\"color\": [151, 0, 95], \"isthing\": 1, \"id\": 49, \"name\": \"knife\"},\n",
        "    {\"color\": [9, 80, 61], \"isthing\": 1, \"id\": 50, \"name\": \"spoon\"},\n",
        "    {\"color\": [84, 105, 51], \"isthing\": 1, \"id\": 51, \"name\": \"bowl\"},\n",
        "    {\"color\": [74, 65, 105], \"isthing\": 1, \"id\": 52, \"name\": \"banana\"},\n",
        "    {\"color\": [166, 196, 102], \"isthing\": 1, \"id\": 53, \"name\": \"apple\"},\n",
        "    {\"color\": [208, 195, 210], \"isthing\": 1, \"id\": 54, \"name\": \"sandwich\"},\n",
        "    {\"color\": [255, 109, 65], \"isthing\": 1, \"id\": 55, \"name\": \"orange\"},\n",
        "    {\"color\": [0, 143, 149], \"isthing\": 1, \"id\": 56, \"name\": \"broccoli\"},\n",
        "    {\"color\": [179, 0, 194], \"isthing\": 1, \"id\": 57, \"name\": \"carrot\"},\n",
        "    {\"color\": [209, 99, 106], \"isthing\": 1, \"id\": 58, \"name\": \"hot dog\"},\n",
        "    {\"color\": [5, 121, 0], \"isthing\": 1, \"id\": 59, \"name\": \"pizza\"},\n",
        "    {\"color\": [227, 255, 205], \"isthing\": 1, \"id\": 60, \"name\": \"donut\"},\n",
        "    {\"color\": [147, 186, 208], \"isthing\": 1, \"id\": 61, \"name\": \"cake\"},\n",
        "    {\"color\": [153, 69, 1], \"isthing\": 1, \"id\": 62, \"name\": \"chair\"},\n",
        "    {\"color\": [3, 95, 161], \"isthing\": 1, \"id\": 63, \"name\": \"couch\"},\n",
        "    {\"color\": [163, 255, 0], \"isthing\": 1, \"id\": 64, \"name\": \"potted plant\"},\n",
        "    {\"color\": [119, 0, 170], \"isthing\": 1, \"id\": 65, \"name\": \"bed\"},\n",
        "    {\"color\": [0, 182, 199], \"isthing\": 1, \"id\": 67, \"name\": \"dining table\"},\n",
        "    {\"color\": [0, 165, 120], \"isthing\": 1, \"id\": 70, \"name\": \"toilet\"},\n",
        "    {\"color\": [183, 130, 88], \"isthing\": 1, \"id\": 72, \"name\": \"tv\"},\n",
        "    {\"color\": [95, 32, 0], \"isthing\": 1, \"id\": 73, \"name\": \"laptop\"},\n",
        "    {\"color\": [130, 114, 135], \"isthing\": 1, \"id\": 74, \"name\": \"mouse\"},\n",
        "    {\"color\": [110, 129, 133], \"isthing\": 1, \"id\": 75, \"name\": \"remote\"},\n",
        "    {\"color\": [166, 74, 118], \"isthing\": 1, \"id\": 76, \"name\": \"keyboard\"},\n",
        "    {\"color\": [219, 142, 185], \"isthing\": 1, \"id\": 77, \"name\": \"cell phone\"},\n",
        "    {\"color\": [79, 210, 114], \"isthing\": 1, \"id\": 78, \"name\": \"microwave\"},\n",
        "    {\"color\": [178, 90, 62], \"isthing\": 1, \"id\": 79, \"name\": \"oven\"},\n",
        "    {\"color\": [65, 70, 15], \"isthing\": 1, \"id\": 80, \"name\": \"toaster\"},\n",
        "    {\"color\": [127, 167, 115], \"isthing\": 1, \"id\": 81, \"name\": \"sink\"},\n",
        "    {\"color\": [59, 105, 106], \"isthing\": 1, \"id\": 82, \"name\": \"refrigerator\"},\n",
        "    {\"color\": [142, 108, 45], \"isthing\": 1, \"id\": 84, \"name\": \"book\"},\n",
        "    {\"color\": [196, 172, 0], \"isthing\": 1, \"id\": 85, \"name\": \"clock\"},\n",
        "    {\"color\": [95, 54, 80], \"isthing\": 1, \"id\": 86, \"name\": \"vase\"},\n",
        "    {\"color\": [128, 76, 255], \"isthing\": 1, \"id\": 87, \"name\": \"scissors\"},\n",
        "    {\"color\": [201, 57, 1], \"isthing\": 1, \"id\": 88, \"name\": \"teddy bear\"},\n",
        "    {\"color\": [246, 0, 122], \"isthing\": 1, \"id\": 89, \"name\": \"hair drier\"},\n",
        "    {\"color\": [191, 162, 208], \"isthing\": 1, \"id\": 90, \"name\": \"toothbrush\"},\n",
        "    {\"color\": [255, 255, 128], \"isthing\": 0, \"id\": 92, \"name\": \"banner\"},\n",
        "    {\"color\": [147, 211, 203], \"isthing\": 0, \"id\": 93, \"name\": \"blanket\"},\n",
        "    {\"color\": [150, 100, 100], \"isthing\": 0, \"id\": 95, \"name\": \"bridge\"},\n",
        "    {\"color\": [168, 171, 172], \"isthing\": 0, \"id\": 100, \"name\": \"cardboard\"},\n",
        "    {\"color\": [146, 112, 198], \"isthing\": 0, \"id\": 107, \"name\": \"counter\"},\n",
        "    {\"color\": [210, 170, 100], \"isthing\": 0, \"id\": 109, \"name\": \"curtain\"},\n",
        "    {\"color\": [92, 136, 89], \"isthing\": 0, \"id\": 112, \"name\": \"door-stuff\"},\n",
        "    {\"color\": [218, 88, 184], \"isthing\": 0, \"id\": 118, \"name\": \"floor-wood\"},\n",
        "    {\"color\": [241, 129, 0], \"isthing\": 0, \"id\": 119, \"name\": \"flower\"},\n",
        "    {\"color\": [217, 17, 255], \"isthing\": 0, \"id\": 122, \"name\": \"fruit\"},\n",
        "    {\"color\": [124, 74, 181], \"isthing\": 0, \"id\": 125, \"name\": \"gravel\"},\n",
        "    {\"color\": [70, 70, 70], \"isthing\": 0, \"id\": 128, \"name\": \"house\"},\n",
        "    {\"color\": [255, 228, 255], \"isthing\": 0, \"id\": 130, \"name\": \"light\"},\n",
        "    {\"color\": [154, 208, 0], \"isthing\": 0, \"id\": 133, \"name\": \"mirror-stuff\"},\n",
        "    {\"color\": [193, 0, 92], \"isthing\": 0, \"id\": 138, \"name\": \"net\"},\n",
        "    {\"color\": [76, 91, 113], \"isthing\": 0, \"id\": 141, \"name\": \"pillow\"},\n",
        "    {\"color\": [255, 180, 195], \"isthing\": 0, \"id\": 144, \"name\": \"platform\"},\n",
        "    {\"color\": [106, 154, 176], \"isthing\": 0, \"id\": 145, \"name\": \"playingfield\"},\n",
        "    {\"color\": [230, 150, 140], \"isthing\": 0, \"id\": 147, \"name\": \"railroad\"},\n",
        "    {\"color\": [60, 143, 255], \"isthing\": 0, \"id\": 148, \"name\": \"river\"},\n",
        "    {\"color\": [128, 64, 128], \"isthing\": 0, \"id\": 149, \"name\": \"road\"},\n",
        "    {\"color\": [92, 82, 55], \"isthing\": 0, \"id\": 151, \"name\": \"roof\"},\n",
        "    {\"color\": [254, 212, 124], \"isthing\": 0, \"id\": 154, \"name\": \"sand\"},\n",
        "    {\"color\": [73, 77, 174], \"isthing\": 0, \"id\": 155, \"name\": \"sea\"},\n",
        "    {\"color\": [255, 160, 98], \"isthing\": 0, \"id\": 156, \"name\": \"shelf\"},\n",
        "    {\"color\": [255, 255, 255], \"isthing\": 0, \"id\": 159, \"name\": \"snow\"},\n",
        "    {\"color\": [104, 84, 109], \"isthing\": 0, \"id\": 161, \"name\": \"stairs\"},\n",
        "    {\"color\": [169, 164, 131], \"isthing\": 0, \"id\": 166, \"name\": \"tent\"},\n",
        "    {\"color\": [225, 199, 255], \"isthing\": 0, \"id\": 168, \"name\": \"towel\"},\n",
        "    {\"color\": [137, 54, 74], \"isthing\": 0, \"id\": 171, \"name\": \"wall-brick\"},\n",
        "    {\"color\": [135, 158, 223], \"isthing\": 0, \"id\": 175, \"name\": \"wall-stone\"},\n",
        "    {\"color\": [7, 246, 231], \"isthing\": 0, \"id\": 176, \"name\": \"wall-tile\"},\n",
        "    {\"color\": [107, 255, 200], \"isthing\": 0, \"id\": 177, \"name\": \"wall-wood\"},\n",
        "    {\"color\": [58, 41, 149], \"isthing\": 0, \"id\": 178, \"name\": \"water-other\"},\n",
        "    {\"color\": [183, 121, 142], \"isthing\": 0, \"id\": 180, \"name\": \"window-blind\"},\n",
        "    {\"color\": [255, 73, 97], \"isthing\": 0, \"id\": 181, \"name\": \"window-other\"},\n",
        "    {\"color\": [107, 142, 35], \"isthing\": 0, \"id\": 184, \"name\": \"tree-merged\"},\n",
        "    {\"color\": [190, 153, 153], \"isthing\": 0, \"id\": 185, \"name\": \"fence-merged\"},\n",
        "    {\"color\": [146, 139, 141], \"isthing\": 0, \"id\": 186, \"name\": \"ceiling-merged\"},\n",
        "    {\"color\": [70, 130, 180], \"isthing\": 0, \"id\": 187, \"name\": \"sky-other-merged\"},\n",
        "    {\"color\": [134, 199, 156], \"isthing\": 0, \"id\": 188, \"name\": \"cabinet-merged\"},\n",
        "    {\"color\": [209, 226, 140], \"isthing\": 0, \"id\": 189, \"name\": \"table-merged\"},\n",
        "    {\"color\": [96, 36, 108], \"isthing\": 0, \"id\": 190, \"name\": \"floor-other-merged\"},\n",
        "    {\"color\": [96, 96, 96], \"isthing\": 0, \"id\": 191, \"name\": \"pavement-merged\"},\n",
        "    {\"color\": [64, 170, 64], \"isthing\": 0, \"id\": 192, \"name\": \"mountain-merged\"},\n",
        "    {\"color\": [152, 251, 152], \"isthing\": 0, \"id\": 193, \"name\": \"grass-merged\"},\n",
        "    {\"color\": [208, 229, 228], \"isthing\": 0, \"id\": 194, \"name\": \"dirt-merged\"},\n",
        "    {\"color\": [206, 186, 171], \"isthing\": 0, \"id\": 195, \"name\": \"paper-merged\"},\n",
        "    {\"color\": [152, 161, 64], \"isthing\": 0, \"id\": 196, \"name\": \"food-other-merged\"},\n",
        "    {\"color\": [116, 112, 0], \"isthing\": 0, \"id\": 197, \"name\": \"building-other-merged\"},\n",
        "    {\"color\": [0, 114, 143], \"isthing\": 0, \"id\": 198, \"name\": \"rock-merged\"},\n",
        "    {\"color\": [102, 102, 156], \"isthing\": 0, \"id\": 199, \"name\": \"wall-other-merged\"},\n",
        "    {\"color\": [250, 141, 255], \"isthing\": 0, \"id\": 200, \"name\": \"rug-merged\"},\n",
        "]\n",
        "\n",
        "categories_for_pp_project = [{'color': [220, 20, 60], 'isthing': 0, 'id': 1, 'name': 'misc'},\n",
        "            {'color': [255, 255, 128], 'isthing': 0, 'id': 2, 'name': 'textile'},\n",
        "            {'color': [150, 100, 100], 'isthing': 0, 'id': 3, 'name': 'building'},\n",
        "            {'color': [168, 171, 172], 'isthing': 0, 'id': 4, 'name': 'rawmaterial'},\n",
        "            {'color': [146, 112, 198], 'isthing': 0, 'id': 5, 'name': 'furniture'},\n",
        "            {'color': [218, 88, 184], 'isthing': 0, 'id': 6, 'name': 'floor'},\n",
        "            {'color': [241, 129, 0], 'isthing': 0, 'id': 7, 'name': 'plant'},\n",
        "            {'color': [217, 17, 255], 'isthing': 0, 'id': 8, 'name': 'food'},\n",
        "            {'color': [124, 74, 181], 'isthing': 0, 'id': 9, 'name': 'ground'},\n",
        "            {'color': [193, 0, 92], 'isthing': 0, 'id': 10, 'name': 'structural'},\n",
        "            {'color': [60, 143, 255], 'isthing': 0, 'id': 11, 'name': 'water'},\n",
        "            {'color': [137, 54, 74], 'isthing': 0, 'id': 12, 'name': 'wall'},\n",
        "            {'color': [183, 121, 142], 'isthing': 0, 'id': 13, 'name': 'window'},\n",
        "            {'color': [146, 139, 141], 'isthing': 0, 'id': 14, 'name': 'ceiling'},\n",
        "            {'color': [70, 130, 180], 'isthing': 0, 'id': 15, 'name': 'sky'},\n",
        "            {'color': [64, 170, 64], 'isthing': 0, 'id': 16, 'name': 'solid'}]\n",
        "\n",
        "old_to_new_category_mapping = {1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1,\n",
        "           19: 1, 20: 1, 21: 1, 22: 1, 23: 1, 24: 1, 25: 1, 27: 1, 28: 1, 31: 1, 32: 1, 33: 1, 34: 1, 35: 1, 36: 1,\n",
        "           37: 1, 38: 1, 39: 1, 40: 1, 41: 1, 42: 1, 43: 1, 44: 1, 46: 1, 47: 1, 48: 1, 49: 1, 50: 1, 51: 1, 52: 1,\n",
        "           53: 1, 54: 1, 55: 1, 56: 1, 57: 1, 58: 1, 59: 1, 60: 1, 61: 1, 62: 1, 63: 1, 64: 1, 65: 1, 67: 1, 70: 1,\n",
        "           72: 1, 73: 1, 74: 1, 75: 1, 76: 1, 77: 1, 78: 1, 79: 1, 80: 1, 81: 1, 82: 1, 84: 1, 85: 1, 86: 1, 87: 1,\n",
        "           88: 1, 89: 1, 90: 1, 92: 2, 93: 2, 95: 3, 100: 4, 107: 5, 109: 2, 112: 5, 118: 6, 119: 7, 122: 8, 125: 9,\n",
        "           128: 3, 130: 5, 133: 5, 138: 10, 141: 2, 144: 9, 145: 9, 147: 9, 148: 11, 149: 9, 151: 3, 154: 9, 155: 11,\n",
        "           156: 5, 159: 9, 161: 5, 166: 3, 168: 2, 171: 12, 175: 12, 176: 12, 177: 12, 178: 11, 180: 13, 181: 13,\n",
        "           184: 7, 185: 10, 186: 14, 187: 15, 188: 5, 189: 5, 190: 6, 191: 9, 192: 16, 193: 7, 194: 9, 195: 4, 196: 8,\n",
        "           197: 3, 198: 16, 199: 12, 200: 2}"
      ],
      "metadata": {
        "id": "ixrocAHwiYxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assign Category Names to COCO classes and \"NA\" to id numbers which are unassigned"
      ],
      "metadata": {
        "id": "lI83x6EOqp8U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coco_category_names = ['N/A'] * 201\n",
        "for c in existing_coco_categories:\n",
        "    coco_category_names[c['id']] = c['name']"
      ],
      "metadata": {
        "id": "UYPezJB2pmXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## List of random colours\n",
        "\n",
        "- Existing COCO classes have unique RGB value to represent a category in MASK Image\n",
        "- But, below is the list of random colours which will be assigned to construction classes"
      ],
      "metadata": {
        "id": "eq-ky8kFq0bZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# since we are treating all things as misc and that belongs to single color class, we can use colors of other things\n",
        "available_colors_for_new_categories = [\n",
        "    [119, 11, 32], \n",
        "    [0, 0, 142], \n",
        "    [0, 0, 230], \n",
        "    [106, 0, 228], \n",
        "    [0, 60, 100],\n",
        "    [0, 80, 100], \n",
        "    [0, 0, 70],\n",
        "    [0, 0, 192], \n",
        "    [250, 170, 30], \n",
        "    [100, 170, 30], \n",
        "    [220, 220, 0], \n",
        "    [175, 116, 175], \n",
        "    [250, 0, 30],\n",
        "    [165, 42, 42], \n",
        "    [255, 77, 255], \n",
        "    [0, 226, 252], \n",
        "    [182, 182, 255], \n",
        "    [0, 82, 0], \n",
        "    [120, 166, 157],\n",
        "    [110, 76, 0], \n",
        "    [174, 57, 255], \n",
        "    [199, 100, 0], \n",
        "    [72, 0, 118], \n",
        "    [255, 179, 240], \n",
        "    [0, 125, 92],\n",
        "    [209, 0, 151], \n",
        "    [188, 208, 182], \n",
        "    [0, 220, 176],\n",
        "    [255, 99, 164], \n",
        "    [92, 0, 73], \n",
        "    [133, 129, 255],\n",
        "    [78, 180, 255], \n",
        "    [0, 228, 0], \n",
        "    [174, 255, 243], \n",
        "    [45, 89, 255], \n",
        "    [134, 134, 103], \n",
        "    [145, 148, 174],\n",
        "    [255, 208, 186], \n",
        "    [197, 226, 255], \n",
        "    [171, 134, 1], \n",
        "    [109, 63, 54], \n",
        "    [207, 138, 255], \n",
        "    [151, 0, 95],\n",
        "    [9, 80, 61], \n",
        "    [84, 105, 51], \n",
        "    [74, 65, 105], \n",
        "    [166, 196, 102], \n",
        "    [208, 195, 210], \n",
        "    [255, 109, 65],\n",
        "    [0, 143, 149], \n",
        "    [179, 0, 194], \n",
        "    [209, 99, 106], \n",
        "    [5, 121, 0], \n",
        "    [227, 255, 205]\n",
        "]"
      ],
      "metadata": {
        "id": "zRIYw0wmjIHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Consturcution CLasses\n",
        "- List of all construction classes on which the model will be trained.\n",
        "\n"
      ],
      "metadata": {
        "id": "q1x-s5pKrHlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_categories = [\n",
        "    \"aac_blocks\",\n",
        "    \"adhesives\",\n",
        "    \"ahus\",\n",
        "    \"aluminium_frames_for_false_ceiling\",\n",
        "    \"chiller\",\n",
        "    \"concrete_mixer_machine\",\n",
        "    \"concrete_pump\",\n",
        "    \"control_panel\",\n",
        "    \"cu_piping\",\n",
        "    \"distribution_transformer\",\n",
        "    \"dump_truck_tipper_truck\",\n",
        "    \"emulsion_paint\",\n",
        "    \"enamel_paint\",\n",
        "    \"fine_aggregate\",\n",
        "    \"fire_buckets\",\n",
        "    \"fire_extinguishers\",\n",
        "    \"glass_wool\",\n",
        "    \"grader\",\n",
        "    \"hoist\",\n",
        "    \"hollow_concrete_blocks\",\n",
        "    \"hot_mix_plant\",\n",
        "    \"hydra_crane\",\n",
        "    \"interlocked_switched_socket\",\n",
        "    \"junction_box\",\n",
        "    \"lime\",\n",
        "    \"marble\",\n",
        "    \"metal_primer\",\n",
        "    \"pipe_fittings\",\n",
        "    \"rcc_hume_pipes\",\n",
        "    \"refrigerant_gas\",\n",
        "    \"river_sand\",\n",
        "    \"rmc_batching_plant\",\n",
        "    \"rmu_units\",\n",
        "    \"sanitary_fixtures\",\n",
        "    \"skid_steer_loader\",\n",
        "    \"smoke_detectors\",\n",
        "    \"split_units\",\n",
        "    \"structural_steel_channel\",\n",
        "    \"switch_boards_and_switches\",\n",
        "    \"texture_paint\",\n",
        "    \"threaded_rod\",\n",
        "    \"transit_mixer\",\n",
        "    \"vcb_panel\",\n",
        "    \"vitrified_tiles\",\n",
        "    \"vrf_units\",\n",
        "    \"water_tank\",\n",
        "    \"wheel_loader\",\n",
        "    \"wood_primer\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "lx_DTls_jJoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Categories\n",
        "\n",
        "- Merge existing COCO classes (our format) + Construction classes\n",
        "- Now we have final list of classes on which our model will be trained"
      ],
      "metadata": {
        "id": "tcQjhbmErQIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category_id = 17\n",
        "available_color_id = 0\n",
        "for category in new_categories:\n",
        "    categories_for_pp_project.append({'color': available_colors_for_new_categories[available_color_id], 'isthing': 1, 'id': category_id, 'name': category})\n",
        "    category_id += 1\n",
        "    available_color_id += 1\n",
        "\n",
        "category_to_id = {\n",
        "    category['name']: category['id'] for category in categories_for_pp_project\n",
        "}\n",
        "\n",
        "id_to_category = {\n",
        "    id: name for id, name in category_to_id.items()\n",
        "}\n",
        "id_to_category"
      ],
      "metadata": {
        "id": "TiQW4RQqjXQj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7035c153-7630-4596-f1da-bc62d2fe7f3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'aac_blocks': 17,\n",
              " 'adhesives': 18,\n",
              " 'ahus': 19,\n",
              " 'aluminium_frames_for_false_ceiling': 20,\n",
              " 'building': 3,\n",
              " 'ceiling': 14,\n",
              " 'chiller': 21,\n",
              " 'concrete_mixer_machine': 22,\n",
              " 'concrete_pump': 23,\n",
              " 'control_panel': 24,\n",
              " 'cu_piping': 25,\n",
              " 'distribution_transformer': 26,\n",
              " 'dump_truck_tipper_truck': 27,\n",
              " 'emulsion_paint': 28,\n",
              " 'enamel_paint': 29,\n",
              " 'fine_aggregate': 30,\n",
              " 'fire_buckets': 31,\n",
              " 'fire_extinguishers': 32,\n",
              " 'floor': 6,\n",
              " 'food': 8,\n",
              " 'furniture': 5,\n",
              " 'glass_wool': 33,\n",
              " 'grader': 34,\n",
              " 'ground': 9,\n",
              " 'hoist': 35,\n",
              " 'hollow_concrete_blocks': 36,\n",
              " 'hot_mix_plant': 37,\n",
              " 'hydra_crane': 38,\n",
              " 'interlocked_switched_socket': 39,\n",
              " 'junction_box': 40,\n",
              " 'lime': 41,\n",
              " 'marble': 42,\n",
              " 'metal_primer': 43,\n",
              " 'misc': 1,\n",
              " 'pipe_fittings': 44,\n",
              " 'plant': 7,\n",
              " 'rawmaterial': 4,\n",
              " 'rcc_hume_pipes': 45,\n",
              " 'refrigerant_gas': 46,\n",
              " 'river_sand': 47,\n",
              " 'rmc_batching_plant': 48,\n",
              " 'rmu_units': 49,\n",
              " 'sanitary_fixtures': 50,\n",
              " 'skid_steer_loader': 51,\n",
              " 'sky': 15,\n",
              " 'smoke_detectors': 52,\n",
              " 'solid': 16,\n",
              " 'split_units': 53,\n",
              " 'structural': 10,\n",
              " 'structural_steel_channel': 54,\n",
              " 'switch_boards_and_switches': 55,\n",
              " 'textile': 2,\n",
              " 'texture_paint': 56,\n",
              " 'threaded_rod': 57,\n",
              " 'transit_mixer': 58,\n",
              " 'vcb_panel': 59,\n",
              " 'vitrified_tiles': 60,\n",
              " 'vrf_units': 61,\n",
              " 'wall': 12,\n",
              " 'water': 11,\n",
              " 'water_tank': 62,\n",
              " 'wheel_loader': 63,\n",
              " 'window': 13,\n",
              " 'wood_primer': 64}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "t6gFcHBirhdd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions for Dataset Creation\n",
        "\n",
        "- Resize Binary Mask\n",
        "- Close the Contour\n",
        "- Binary Mask to Polygon\n",
        "- Create Image Data and JSON\n",
        "- Create Annotation Data"
      ],
      "metadata": {
        "id": "fmcuC6mdSx0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "import os\n",
        "import re\n",
        "import datetime\n",
        "import numpy as np\n",
        "from itertools import groupby\n",
        "from skimage import measure\n",
        "from PIL import Image\n",
        "from pycocotools import mask\n",
        "\n",
        "convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
        "natrual_key = lambda key: [ convert(c) for c in re.split('([0-9]+)', key) ]\n",
        "\n",
        "def resize_binary_mask(array, new_size):\n",
        "    image = Image.fromarray(array.astype(np.uint8)*255)\n",
        "    image = image.resize(new_size)\n",
        "    return np.asarray(image).astype(np.bool_)\n",
        "\n",
        "def close_contour(contour):\n",
        "    if not np.array_equal(contour[0], contour[-1]):\n",
        "        contour = np.vstack((contour, contour[0]))\n",
        "    return contour\n",
        "\n",
        "def binary_mask_to_rle(binary_mask):\n",
        "    rle = {'counts': [], 'size': list(binary_mask.shape)}\n",
        "    counts = rle.get('counts')\n",
        "    for i, (value, elements) in enumerate(groupby(binary_mask.ravel(order='F'))):\n",
        "        if i == 0 and value == 1:\n",
        "                counts.append(0)\n",
        "        counts.append(len(list(elements)))\n",
        "\n",
        "    return rle\n",
        "\n",
        "def binary_mask_to_polygon(binary_mask, tolerance=0):\n",
        "    \"\"\"Converts a binary mask to COCO polygon representation\n",
        "\n",
        "    Args:\n",
        "        binary_mask: a 2D binary numpy array where '1's represent the object\n",
        "        tolerance: Maximum distance from original points of polygon to approximated\n",
        "            polygonal chain. If tolerance is 0, the original coordinate array is returned.\n",
        "\n",
        "    \"\"\"\n",
        "    polygons = []\n",
        "    # pad mask to close contours of shapes which start and end at an edge\n",
        "    padded_binary_mask = np.pad(binary_mask, pad_width=1, mode='constant', constant_values=0)\n",
        "    contours = measure.find_contours(padded_binary_mask, 0.5)\n",
        "    contours = np.subtract(contours, 1)\n",
        "    for contour in contours:\n",
        "        contour = close_contour(contour)\n",
        "        contour = measure.approximate_polygon(contour, tolerance)\n",
        "        if len(contour) < 3:\n",
        "            continue\n",
        "        contour = np.flip(contour, axis=1)\n",
        "        segmentation = contour.ravel().tolist()\n",
        "        # after padding and subtracting 1 we may get -0.5 points in our segmentation \n",
        "        segmentation = [0 if i < 0 else i for i in segmentation]\n",
        "        polygons.append(segmentation)\n",
        "\n",
        "    return polygons\n",
        "\n",
        "def create_image_info(image_id, file_name, image_size, \n",
        "                      date_captured=datetime.datetime.utcnow().isoformat(' '),\n",
        "                      license_id=1, coco_url=\"\", flickr_url=\"\"):\n",
        "\n",
        "    image_info = {\n",
        "            \"id\": image_id,\n",
        "            \"file_name\": file_name,\n",
        "            \"width\": image_size[0],\n",
        "            \"height\": image_size[1],\n",
        "            \"date_captured\": date_captured,\n",
        "            \"license\": license_id,\n",
        "            \"coco_url\": coco_url,\n",
        "            \"flickr_url\": flickr_url\n",
        "    }\n",
        "\n",
        "    return image_info\n",
        "\n",
        "def create_annotation_info(annotation_id, image_id, category_info, binary_mask, \n",
        "                           image_size=None, tolerance=2, bounding_box=None):\n",
        "\n",
        "    if image_size is not None:\n",
        "        binary_mask = resize_binary_mask(binary_mask, image_size)\n",
        "\n",
        "    binary_mask_encoded = mask.encode(np.asfortranarray(binary_mask.astype(np.uint8)))\n",
        "\n",
        "    area = mask.area(binary_mask_encoded)\n",
        "    if area < 1:\n",
        "        return None\n",
        "\n",
        "    if bounding_box is None:\n",
        "        bounding_box = mask.toBbox(binary_mask_encoded)\n",
        "\n",
        "    if category_info[\"is_crowd\"]:\n",
        "        is_crowd = 1\n",
        "        segmentation = binary_mask_to_rle(binary_mask)\n",
        "    else :\n",
        "        is_crowd = 0\n",
        "        segmentation = binary_mask_to_polygon(binary_mask, tolerance)\n",
        "        if not segmentation:\n",
        "            return None\n",
        "\n",
        "    annotation_info = {\n",
        "        \"id\": annotation_id,\n",
        "        \"image_id\": image_id,\n",
        "        \"category_id\": category_info[\"id\"],\n",
        "        \"iscrowd\": is_crowd,\n",
        "        \"area\": area.tolist(),\n",
        "        \"bbox\": bounding_box.tolist(),\n",
        "        \"segmentation\": segmentation,\n",
        "        \"width\": binary_mask.shape[1],\n",
        "        \"height\": binary_mask.shape[0],\n",
        "    } \n",
        "\n",
        "    return annotation_info"
      ],
      "metadata": {
        "id": "uuykHZsR6f9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Super Impose Function\n",
        "\n",
        "- To overlay our construction materials on top of COCO output predicted by Facebook DETR"
      ],
      "metadata": {
        "id": "Tyvm8lOtTD5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from math import floor\n",
        "\n",
        "def superimpose_thing(image_size, annotations):\n",
        "    height, width = image_size\n",
        "\n",
        "    # create a single channel black image\n",
        "    superimposed_image = np.zeros((height, width))\n",
        "\n",
        "    polygons_list = []\n",
        "    # Add the polygon segmentation\n",
        "    for segmentation_points in annotation['segmentation']:\n",
        "        segmentation_points = np.multiply(segmentation_points, 1).astype(int)\n",
        "        polygons_list.append(segmentation_points)\n",
        "\n",
        "    # convert segmentation points to contour\n",
        "    for x in polygons_list:\n",
        "        end = []\n",
        "        if len(x) % 2 != 0:\n",
        "            print(x)\n",
        "        for l in range(0, len(x), 2):\n",
        "            coords = [floor(x[l]), floor(x[l + 1])]\n",
        "            end.append(coords)\n",
        "        contours = np.array(end)\n",
        "        if end == []:\n",
        "            continue\n",
        "\n",
        "        # plot and fill the contour\n",
        "        cv2.fillPoly(superimposed_image, pts=[contours], color=(1, 1, 1))\n",
        "  \n",
        "    return superimposed_image"
      ],
      "metadata": {
        "id": "8P7MDBIC6hfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pycocotools import mask\n",
        "from skimage import measure\n",
        "\n",
        "def make_contours_closed(contour):\n",
        "    if not np.array_equal(contour[0], contour[-1]):\n",
        "        contour = np.vstack((contour, contour[0]))\n",
        "    return contour\n",
        "    \n",
        "\n",
        "def binary_mask_to_polygon(binary_mask, tolerance=0):\n",
        "    \"\"\"Converts a binary mask to COCO polygon representation\n",
        "\n",
        "    Args:\n",
        "        binary_mask: a 2D binary numpy array where '1's represent the object\n",
        "        tolerance: Maximum distance from original points of polygon to approximated\n",
        "            polygonal chain. If tolerance is 0, the original coordinate array is returned.\n",
        "\n",
        "    \"\"\"\n",
        "    polygons = []\n",
        "    # pad mask to close contours of shapes which start and end at an edge\n",
        "    padded_binary_mask = np.pad(binary_mask, pad_width=1, mode='constant', constant_values=0)\n",
        "    contours = measure.find_contours(padded_binary_mask, 0.5)\n",
        "    contours = np.subtract(contours, 1)\n",
        "    for contour in contours:\n",
        "        contour = make_contours_closed(contour)\n",
        "        contour = measure.approximate_polygon(contour, tolerance)\n",
        "        if len(contour) < 3:\n",
        "            continue\n",
        "        contour = np.flip(contour, axis=1)\n",
        "        segmentation = contour.ravel().tolist()\n",
        "        # after padding and subtracting 1 we may get -0.5 points in our segmentation \n",
        "        segmentation = [0 if i < 0 else i for i in segmentation]\n",
        "        polygons.append(segmentation)\n",
        "\n",
        "    return polygons\n",
        "\n",
        "def convert(o):\n",
        "    if isinstance(o, np.generic): return o.item()  \n",
        "    raise TypeError\n",
        "\n",
        "\n",
        "\n",
        "def get_annotation_info(binary_mask, image_size, image_id, class_id, segmentation_id, iscrowd):\n",
        "\n",
        "    category_info = {'id': class_id, 'is_crowd': iscrowd}\n",
        "\n",
        "    tolerance = 2\n",
        "    bounding_box = None    \n",
        "\n",
        "    if image_size is not None:\n",
        "        binary_mask = resize_binary_mask(binary_mask, image_size)\n",
        "\n",
        "    binary_mask_encoded = mask.encode(np.asfortranarray(binary_mask.astype(np.uint8)))\n",
        "\n",
        "    area = mask.area(binary_mask_encoded)\n",
        "    if area < 1:\n",
        "        return None\n",
        "\n",
        "    if bounding_box is None:\n",
        "        bounding_box = mask.toBbox(binary_mask_encoded)\n",
        "\n",
        "    if category_info[\"is_crowd\"]:\n",
        "        is_crowd = 1\n",
        "        segmentation = binary_mask_to_rle(binary_mask)\n",
        "    else :\n",
        "        is_crowd = 0\n",
        "        segmentation = binary_mask_to_polygon(binary_mask, tolerance)\n",
        "        if not segmentation:\n",
        "            return None\n",
        "\n",
        "    annotation_info = {\n",
        "        \"id\": annotation_id,\n",
        "        \"image_id\": image_id,\n",
        "        \"category_id\": category_info[\"id\"],\n",
        "        \"iscrowd\": is_crowd,\n",
        "        \"area\": area.tolist(),\n",
        "        \"bbox\": bounding_box.tolist(),\n",
        "        \"segmentation\": segmentation,\n",
        "        \"width\": binary_mask.shape[1],\n",
        "        \"height\": binary_mask.shape[0],\n",
        "    } \n",
        "\n",
        "\n",
        "    return annotation_info"
      ],
      "metadata": {
        "id": "c5BPhDNb6psj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import and Load FACEBOOK DETR Pre-Trained Model"
      ],
      "metadata": {
        "id": "skOWfDSKTOdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as T\n",
        "\n",
        "# standard PyTorch mean-std input image normalization\n",
        "transform = T.Compose([\n",
        "    T.Resize(800),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load detr model\n",
        "model, postprocessor = torch.hub.load('detr', 'detr_resnet101_panoptic', source='local', pretrained=True, return_postprocessor=True, num_classes=250)\n",
        "# Convert to eval mode\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"Model Loaded\")\n",
        "\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150,
          "referenced_widgets": [
            "aeb29033d1ae48e6a8ff12d926a26e9e",
            "0eec2c8647af42dd86dda3754fbfa44f",
            "cb69729e5e5f4c5db403f06b98f5cd07",
            "04f72e2acaa9428baee882719a032ece",
            "a981d9fe0811408e91520ec08ff07084",
            "5b786291b3b142bb8f4edc823a6b7cbb",
            "b5f0c7e6ce234c28948d003f71dcafc2",
            "1d6a343d22c2433cb9f3ffaf958b4538",
            "6e35a00e727849d8ac56c7a88f99c49e",
            "dddcaa9ea4274953b8a874bc3258cbad",
            "3f93a63605ce407d9823877cef09008e",
            "41aab71fb0134c659e4d415fe1e0ef7a",
            "a072b4e6ba4b40d6a9c59bc2ccc34963",
            "dbac77706d7c411dadbabe8b529fa09a",
            "8bd5bc89239e42cba5a45c56a0fc4834",
            "1f1bf0f27c5f4ea48fe6bff5f13f8b82",
            "bf27be1e62d94a8db42660d7f47ad40a",
            "4cf3900d712149868884dea8b4536782",
            "675e00ea08cd49d1a49f25b59772598c",
            "00244c4b97a64066af4a1d17dd85b17f",
            "cfa08b26a24640baa3ce906202e5dfb4",
            "a93a9c9ac918429a8d6d26ef01c1f92c"
          ]
        },
        "id": "nOU-lAlttlqw",
        "outputId": "e8ab3e0f-fd15-402f-f5f2-69dc68d1c06c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-63fe2227.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/171M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aeb29033d1ae48e6a8ff12d926a26e9e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/detr/detr-r101-panoptic-40021d53.pth\" to /root/.cache/torch/hub/checkpoints/detr-r101-panoptic-40021d53.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/237M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41aab71fb0134c659e4d415fe1e0ef7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Loaded\n",
            "/content/drive/MyDrive/Panoptic Segmentation using DETR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mask Image Creation and JSON Creation\n",
        "1. For all categories and images\n",
        "2. Pass Image to Facebbok model\n",
        "3. Gather predictions\n",
        "4. COnvert predictions to our mapping\n",
        "5. Overlay construction materials on top of COCO classes\n",
        "6. Adjust Mask images and mappings accordingly\n",
        "7. Save Mask Image and JSON\n"
      ],
      "metadata": {
        "id": "1l4iuemoTUwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bjSIKf0ttr6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "import io\n",
        "import panopticapi\n",
        "from panopticapi.utils import id2rgb, rgb2id\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import numpy\n",
        "torch.set_grad_enabled(False)\n",
        "import json\n",
        "\n",
        "\n",
        "\n",
        "image_id = 1\n",
        "annotation_id = 1\n",
        "segment_id = 1\n",
        "\n",
        "detection_coco = {\n",
        "    \"categories\": categories_for_pp_project,\n",
        "    \"annotations\": [],\n",
        "    \"images\": []\n",
        "}\n",
        "\n",
        "panoptic_coco = {\n",
        "    \"categories\": categories_for_pp_project,\n",
        "    \"annotations\": [],\n",
        "    \"images\": []\n",
        "}\n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/Panoptic Segmentation using DETR/Dataset\")\n",
        "cats = os.listdir()\n",
        "\n",
        "os.makedirs(\"/content/drive/MyDrive/Panoptic Segmentation using DETR/data/train\", exist_ok=True)\n",
        "os.makedirs(\"/content/drive/MyDrive/Panoptic Segmentation using DETR/data/panoptic_train\", exist_ok=True)\n",
        "\n",
        "final_images_path = \"/content/drive/MyDrive/Panoptic Segmentation using DETR/data/\"\n",
        "\n",
        "\n",
        "# run through all folders in dataset\n",
        "for cat in cats:\n",
        "    if os.path.isfile(cat):\n",
        "      continue\n",
        "\n",
        "    # get category name\n",
        "    category_name = cat\n",
        "    print(\"Category:\", category_name)\n",
        "    with open(os.path.join(cat, \"coco.json\"), \"r\") as json_file:\n",
        "        category_json = json.load(json_file)\n",
        "        \n",
        "    images_path = os.path.join(cat, 'images')\n",
        "        \n",
        "    temporary_annotations = {}\n",
        "    \n",
        "    # Run over all images\n",
        "    for image_info in category_json[\"images\"]:\n",
        "        image_info['annotations'] = []\n",
        "        temporary_annotations[image_info['id']] = image_info\n",
        "        \n",
        "    for annnotation_info in category_json[\"annotations\"]:\n",
        "        temporary_annotations[annnotation_info['image_id']][\"annotations\"].append(annnotation_info)\n",
        "        \n",
        "    for i, image_item in temporary_annotations.items():\n",
        "\n",
        "        output_file_name = category_name + \"_\" + str(image_id) + \".jpg\"\n",
        "        output_file_path = os.path.join(final_images_path, \"train\", output_file_name)\n",
        "        \n",
        "        output_mask_name = category_name + \"_\" + str(image_id) + \".png\"\n",
        "        output_mask_path = os.path.join(final_images_path, \"panoptic_train\", output_mask_name)\n",
        "        \n",
        "        try:\n",
        "\n",
        "            # Read the image and get shape of image\n",
        "            original_image = Image.open(os.path.join(images_path, image_item['file_name'])).convert('RGB')\n",
        "\n",
        "            try:\n",
        "                h, w, c = np.array(original_image).shape\n",
        "            except:\n",
        "                h, w = np.array(original_image).shape\n",
        "                c = 1\n",
        "\n",
        "            # if no of channels != 3, open the image and convert it to 3 channel - RGB\n",
        "            if c == 4 or c == 1:\n",
        "                original_image = original_image.convert('RGB')\n",
        "                h, w, c = np.array(original_image).shape\n",
        "\n",
        "            duplicate_image = original_image.copy()\n",
        "\n",
        "            # Apply transform and convert image to batch\n",
        "            # mean-std normalize the input image (batch-size: 1)\n",
        "            img = transform(duplicate_image).unsqueeze(0).to(device)  # [h, w, c] -> [1, c, ht, wt]\n",
        "\n",
        "            # Generate output for image\n",
        "            out = model(img)\n",
        "\n",
        "            # Generate score\n",
        "            # compute the scores, excluding the \"no-object\" class (the last one)\n",
        "            scores = out[\"pred_logits\"].softmax(-1)[..., :-1].max(-1)[0]\n",
        "\n",
        "            # threshold the confidence\n",
        "            keep = scores > 0.85\n",
        "\n",
        "            # Keep only ones above threshold\n",
        "            pred_logits, pred_boxes = out[\"pred_logits\"][keep][:, :len(\n",
        "                coco_category_names) - 1], out[\"pred_boxes\"][keep]\n",
        "\n",
        "            # the post-processor expects as input the target size of the predictions (which we set here to the image size)\n",
        "            result = postprocessor(out, torch.as_tensor(img.shape[-2:]).unsqueeze(0))[0]\n",
        "\n",
        "            # The segmentation is stored in a special-format png\n",
        "            panoptic_seg = Image.open(io.BytesIO(result['png_string'])).resize((w, h), Image.NEAREST)\n",
        "            # (wp, hp) = panoptic_seg.size\n",
        "            panoptic_seg = np.array(panoptic_seg, dtype=np.uint8).copy()\n",
        "\n",
        "            # We retrieve the ids corresponding to each mask\n",
        "            panoptic_seg_id = rgb2id(panoptic_seg)\n",
        "            \n",
        "            # get unique prediction ids\n",
        "            unique_category_id = []\n",
        "            for i, segment in enumerate(result['segments_info']):\n",
        "                result['segments_info'][i][\"category_id\"] = old_to_new_category_mapping[result['segments_info'][i][\"category_id\"]]\n",
        "                if result['segments_info'][i][\"category_id\"] not in unique_category_id:\n",
        "                    unique_category_id.append(result['segments_info'][i][\"category_id\"])\n",
        "            \n",
        "            # Sort array\n",
        "            unique_category_id.sort()\n",
        "            \n",
        "            unique_category_id_to_id =  {category_id: i for i, category_id in enumerate(unique_category_id)}\n",
        "            unique_id_to_category_id =  {i: category_id for category_id, i in unique_category_id_to_id.items()}\n",
        "            \n",
        "            for i, segment in enumerate(result['segments_info']):\n",
        "                result['segments_info'][i][\"new_id\"] = unique_category_id_to_id[result['segments_info'][i][\"category_id\"]]\n",
        "            \n",
        "            # Update original panoptic_seg_id array with new ids as the new segmentation combines different categories.\n",
        "            custom_panoptic_seg_id = np.zeros((panoptic_seg_id.shape[0], panoptic_seg_id.shape[1]), dtype=np.uint8)\n",
        "            \n",
        "            # Update this custom panoptic seg matrix\n",
        "            for i, segment in enumerate(result['segments_info']):\n",
        "                custom_panoptic_seg_id[result['segments_info'][i]['id'] == panoptic_seg_id] = result['segments_info'][i]['new_id']\n",
        "            \n",
        "            custom_panoptic_segments_info = []\n",
        "            for category_id in unique_category_id:\n",
        "                custom_panoptic_segments_info.append({\n",
        "                    'segment_id': unique_category_id_to_id[category_id], \n",
        "                    'category_id': category_id,\n",
        "                    'bbox': [],\n",
        "                    'area': 0,\n",
        "                    'iscrowd': 0,\n",
        "                    'isthing': 0\n",
        "                })\n",
        "\n",
        "            # annotations of our construction things\n",
        "            original_mask = image_item['annotations']\n",
        "            \n",
        "            to_append_annotations = []\n",
        "            \n",
        "            # Overlay things mask one at a time\n",
        "            for annotation in original_mask:\n",
        "                # overlay mask of construction things on top of detr output\n",
        "                omask_image_id = superimpose_thing((h, w), annotation)\n",
        "                custom_panoptic_seg_id[omask_image_id.astype(np.bool_)] = custom_panoptic_seg_id.max() + 1\n",
        "                custom_panoptic_segments_info.append({\n",
        "                    'segment_id': custom_panoptic_seg_id.max(), \n",
        "                    'category_id': category_to_id[category_name], \n",
        "                    'bbox': annotation['bbox'],\n",
        "                    'area': annotation['area'],\n",
        "                    'iscrowd': 0,\n",
        "                    'isthing': 1\n",
        "                })\n",
        "\n",
        "                # append annotation of construction things in json file\n",
        "                annotation[\"category_id\"] = category_to_id[category_name]\n",
        "                annotation[\"image_id\"] = image_id\n",
        "                to_append_annotations.append(annotation)\n",
        "            \n",
        "            # Convert to binary segment\n",
        "            binary_masks = np.zeros((\n",
        "                custom_panoptic_seg_id.max() + 1,\n",
        "                custom_panoptic_seg_id.shape[0],\n",
        "                custom_panoptic_seg_id.shape[1]),\n",
        "                dtype=np.uint8\n",
        "            )\n",
        "\n",
        "            # for each binary mask, detect contours and create annotation for those contours\n",
        "            if len(unique_category_id):\n",
        "                # Skip the onse which are added by us\n",
        "                for category_id in unique_category_id:\n",
        "                    binary_masks[unique_category_id_to_id[category_id], :, :] = custom_panoptic_seg_id == unique_category_id_to_id[category_id]\n",
        "                    annotation_info = get_annotation_info(binary_masks[unique_category_id_to_id[category_id]], None, image_id, category_id, unique_category_id_to_id[category_id], 0)\n",
        "                    if annotation_info is not None:\n",
        "                        annotation_info[\"image_id\"] = image_id\n",
        "                        annotation_info[\"category_id\"] = category_id\n",
        "                        to_append_annotations.append(annotation_info)\n",
        "                        \n",
        "                        custom_panoptic_segments_info[unique_category_id_to_id[category_id]]['bbox'] = annotation_info['bbox']\n",
        "                        custom_panoptic_segments_info[unique_category_id_to_id[category_id]]['area'] = annotation_info['area']\n",
        "            else:\n",
        "                pass\n",
        "            \n",
        "            # save image to new path as .jpg\n",
        "            original_image.save(output_file_path)\n",
        "            \n",
        "            # save panoptic image\n",
        "            Image.fromarray(id2rgb(custom_panoptic_seg_id), 'RGB').save(output_mask_path)\n",
        "\n",
        "            # create image_info object and append it to original list\n",
        "            image_info = {\n",
        "                \"id\": image_id,\n",
        "                \"file_name\": output_file_name,\n",
        "                \"width\": original_image.size[0],\n",
        "                \"height\": original_image.size[1]\n",
        "                }\n",
        "            \n",
        "            detection_coco[\"images\"].append(image_info)\n",
        "            panoptic_coco[\"images\"].append(image_info)\n",
        "\n",
        "            for annotation in to_append_annotations:\n",
        "                annotation[\"id\"] = annotation_id\n",
        "                detection_coco[\"annotations\"].append(annotation)\n",
        "                annotation_id += 1\n",
        "                \n",
        "            for segment_info in custom_panoptic_segments_info:\n",
        "                segment_info[\"id\"] = segment_id\n",
        "                segment_id += 1\n",
        "                \n",
        "            panoptic_coco[\"annotations\"].append({\n",
        "                \"segments_info\": custom_panoptic_segments_info,\n",
        "                \"file_name\": output_mask_name,\n",
        "                \"image_id\": image_id\n",
        "            })\n",
        "\n",
        "            # increment the image_count\n",
        "            image_id += 1\n",
        "    \n",
        "        except Exception as e:\n",
        "            # print(\"Error ******** :\", os.path.join(images_path, image_item['file_name']))\n",
        "            continue    \n",
        "\n",
        "    # open the final json, and commit changes in that file\n",
        "    with open(os.path.join(final_images_path, \"train.json\"), 'w') as output_json_file:\n",
        "        json.dump(detection_coco, output_json_file)\n",
        "        \n",
        "    with open(os.path.join(final_images_path, \"panoptic_train.json\"), 'w') as output_json_file:\n",
        "        json.dump(panoptic_coco, output_json_file, default=convert)\n",
        "        \n",
        "    print(image_id, annotation_id, segment_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhGN2YuOtuOJ",
        "outputId": "9182cd8a-74a0-41ca-b3c4-47e9107daf26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Category: control_panel\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "detr/models/position_encoding.py:41: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "246 689 702\n",
            "Category: distribution_transformer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "657 2505 2521\n",
            "Category: chiller\n",
            "702 2655 2675\n",
            "Category: concrete_pump_(50_)\n",
            "702 2655 2675\n",
            "Category: aluminium_frames_for_false_ceiling\n",
            "752 2797 2818\n",
            "Category: ahus\n",
            "885 3161 3188\n",
            "Category: aac_blocks\n",
            "1148 4197 4225\n",
            "Category: concrete_mixer_machine\n",
            "1198 4373 4402\n",
            "Category: adhesives\n",
            "1298 4581 4610\n",
            "Category: cu_piping\n",
            "1798 5959 5995\n",
            "Category: hoist\n",
            "2333 7953 8011\n",
            "Category: dump_truck___tipper_truck\n",
            "2333 7953 8011\n",
            "Category: glass_wool\n",
            "2403 8261 8319\n",
            "Category: fine_aggregate\n",
            "2903 9241 9301\n",
            "Category: fire_extinguishers\n",
            "3131 9867 9930\n",
            "Category: emulsion_paint\n",
            "3163 9946 10011\n",
            "Category: grader\n",
            "3463 11349 11420\n",
            "Category: fire_buckets\n",
            "3921 12802 12880\n",
            "Category: hollow_concrete_blocks\n",
            "3971 12962 13041\n",
            "Category: enamel_paint\n",
            "4022 13082 13163\n",
            "Category: hydra_crane\n",
            "4122 13497 13580\n",
            "Category: marble\n",
            "4172 13682 13768\n",
            "Category: rcc_hume_pipes\n",
            "4412 14394 14483\n",
            "Category: interlocked_switched_socket\n",
            "4512 14550 14642\n",
            "Category: lime\n",
            "5024 16952 17047\n",
            "Category: hot_mix_plant\n",
            "5124 17426 17526\n",
            "Category: junction_box\n",
            "5176 17566 17667\n",
            "Category: refrigerant_gas\n",
            "5274 17895 17997\n",
            "Category: pipe_fittings\n",
            "5328 18011 18113\n",
            "Category: metal_primer\n",
            "5387 18132 18234\n",
            "Category: river_sand\n",
            "5557 18996 19099\n",
            "Category: skid_steer_loader_(bobcat)\n",
            "5560 19008 19111\n",
            "Category: structural_steel_-_channel\n",
            "5561 19009 19112\n",
            "Category: rmc_batching_plant\n",
            "5620 19309 19417\n",
            "Category: switch_boards_and_switches\n",
            "6120 20345 20486\n",
            "Category: smoke_detectors\n",
            "6170 20439 20580\n",
            "Category: texture_paint\n",
            "6222 20542 20685\n",
            "Category: sanitary_fixtures\n",
            "6743 23212 23357\n",
            "Category: split_units\n",
            "7281 24879 25036\n",
            "Category: rmu_units\n",
            "7381 25124 25282\n",
            "Category: water_tank\n",
            "7685 26495 26656\n",
            "Category: wheel_loader\n",
            "7835 27189 27358\n",
            "Category: transit_mixer\n",
            "7885 27438 27607\n",
            "Category: wood_primer\n",
            "7897 27467 27637\n",
            "Category: vrf_units\n",
            "7947 27589 27759\n",
            "Category: vcb_panel\n",
            "8447 29278 29469\n",
            "Category: threaded_rod\n",
            "8998 30617 30808\n",
            "Category: vitrified_tiles\n",
            "9398 32148 32375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train - Test Split\n",
        "\n",
        "- Split bounding box and panoptic segmentation json into 80(train):20(test)\n"
      ],
      "metadata": {
        "id": "Nb2N-vgstAKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/Panoptic Segmentation using DETR/data\")\n",
        "\n",
        "f = open(\"train.json\")\n",
        "train_json = json.load(f)\n",
        "f.close()\n",
        "\n",
        "f = open(\"panoptic_train.json\")\n",
        "panoptic_train_json = json.load(f)\n",
        "f.close()\n",
        "\n",
        "val_json = {}\n",
        "panoptic_val_json = {}\n",
        "\n",
        "\n",
        "train_images = train_json[\"images\"]\n",
        "train_annotations = train_json[\"annotations\"]\n",
        "\n",
        "val_images = []\n",
        "val_annotations = []\n",
        "\n",
        "panoptic_train_images = panoptic_train_json[\"images\"]\n",
        "panoptic_train_annotations = panoptic_train_json[\"annotations\"]\n",
        "\n",
        "panoptic_val_images = []\n",
        "panoptic_val_annotations = []\n",
        "\n",
        "\n",
        "print(len(train_images), len(val_images), len(train_annotations), len(val_annotations), len(panoptic_train_images), len(panoptic_train_annotations), len(panoptic_val_images), len(panoptic_val_annotations))\n",
        "\n",
        "count = 0\n",
        "while True:\n",
        "    if count >= 2000:\n",
        "        break\n",
        "    random.shuffle(train_images)\n",
        "    to_del = train_images[0]\n",
        "    found = False\n",
        "    for x in panoptic_train_images:\n",
        "        if x[\"id\"] == to_del[\"id\"]:\n",
        "            panoptic_val_images.append(x)\n",
        "            panoptic_train_images.remove(x)\n",
        "            found = True\n",
        "    if not found:\n",
        "        print(to_del)\n",
        "    if found:\n",
        "        val_images.append(train_images[0])\n",
        "        for x in panoptic_train_annotations:\n",
        "            if x[\"image_id\"] == to_del[\"id\"]:\n",
        "                panoptic_val_annotations.append(x)\n",
        "                panoptic_train_annotations.remove(x)\n",
        "        for x in train_annotations:\n",
        "            if x[\"image_id\"] == to_del[\"id\"]:\n",
        "                val_annotations.append(x)\n",
        "                train_annotations.remove(x)\n",
        "        train_images.remove(to_del)\n",
        "        count += 1\n",
        "\n",
        "\n",
        "print(len(train_images), len(val_images), len(train_annotations), len(val_annotations), len(panoptic_train_images), len(panoptic_train_annotations), len(panoptic_val_images), len(panoptic_val_annotations))\n",
        "\n",
        "\n",
        "train_json[\"images\"] = train_images\n",
        "train_json[\"annotations\"] = train_annotations\n",
        "\n",
        "\n",
        "val_json[\"images\"] = val_images\n",
        "val_json[\"annotations\"] = val_annotations\n",
        "\n",
        "panoptic_train_json[\"images\"] = panoptic_train_images\n",
        "panoptic_train_json[\"annotations\"] = panoptic_train_annotations\n",
        "\n",
        "panoptic_val_json[\"images\"] = panoptic_val_images\n",
        "panoptic_val_json[\"annotations\"] = panoptic_val_annotations\n",
        "\n",
        "f = open(\"train.json\", \"w\")\n",
        "json.dump(train_json, f)\n",
        "f.close()\n",
        "\n",
        "f = open(\"val.json\", \"w\")\n",
        "json.dump(val_json, f)\n",
        "f.close()\n",
        "\n",
        "f = open(\"panoptic_train.json\", \"w\")\n",
        "json.dump(panoptic_train_json, f)\n",
        "f.close()\n",
        "\n",
        "f = open(\"panoptic_val.json\", \"w\")\n",
        "json.dump(panoptic_val_json, f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "NNdHh9QstBnS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44ba7f9d-ff2c-4a64-d50e-47733cefecc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9397 0 32147 0 9397 9397 0 0\n",
            "7397 2000 28193 3954 7397 7397 2000 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post-process the Dataset\n",
        "\n",
        "- Remove any empty annotations\n",
        "- Remove an image if it has no annotations\n",
        "- Remove an object record - if it does not satisfy our data models"
      ],
      "metadata": {
        "id": "AbeCUi2QFAIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/Panoptic Segmentation using DETR/data\")\n",
        "\n",
        "files = [\"train.json\", \"val.json\"]\n",
        "\n",
        "for file in files:\n",
        "    f = open(file)\n",
        "    j = json.load(f)\n",
        "\n",
        "    for x in j[\"annotations\"]:\n",
        "        if x[\"segmentation\"] == []:\n",
        "            print(x)\n",
        "            j[\"annotations\"].remove(x)\n",
        "\n",
        "    f = open(file, \"w\")\n",
        "    json.dump(j, f)\n",
        "    f.close()\n",
        "\n",
        "\n",
        "\n",
        "files = [\"panoptic_train.json\", \"panoptic_val.json\"]\n",
        "\n",
        "for file in files:\n",
        "    f = open(file)\n",
        "    j = json.load(f)\n",
        "\n",
        "    for x in j[\"annotations\"]:\n",
        "        for y in x[\"segments_info\"]:\n",
        "            if len(y[\"bbox\"]) != 4:\n",
        "                x[\"segments_info\"].remove(y)\n",
        "\n",
        "    f = open(file, \"w\")\n",
        "    json.dump(j, f)\n",
        "    f.close()\n"
      ],
      "metadata": {
        "id": "6Xy5_Z6QITEg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9872f3fd-c6b9-4d77-8081-ec44e776c95d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 4812, 'image_id': 1397, 'category_id': 25, 'segmentation': [], 'area': 50216.113, 'bbox': [0.49, 0.7, 212.51, 236.3], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 8099, 'image_id': 2369, 'category_id': 33, 'segmentation': [], 'area': 688769.9221, 'bbox': [3.24, 2.44, 1017.13, 677.17], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 8225, 'image_id': 2396, 'category_id': 33, 'segmentation': [], 'area': 3890.2464000000036, 'bbox': [0.91, 320.84, 76.64, 50.76], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 8257, 'image_id': 2402, 'category_id': 33, 'segmentation': [], 'area': 28965.138700000018, 'bbox': [270.45, 534.9, 175.09, 165.43], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 17514, 'image_id': 5156, 'category_id': 40, 'segmentation': [], 'area': 32099.641599999995, 'bbox': [425.59, 10.76, 179.84, 178.49], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 17516, 'image_id': 5156, 'category_id': 40, 'segmentation': [], 'area': 17854.636000000006, 'bbox': [441.54, 822.4, 135.16, 132.1], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 17518, 'image_id': 5156, 'category_id': 40, 'segmentation': [], 'area': 17920.364, 'bbox': [123.3, 4.8, 127.82, 140.2], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 17520, 'image_id': 5156, 'category_id': 40, 'segmentation': [], 'area': 23389.778699999995, 'bbox': [102.53, 596.74, 153.87, 152.01], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 17974, 'image_id': 5309, 'category_id': 44, 'segmentation': [], 'area': 9808.3725, 'bbox': [78.23, 61.89, 64.55, 151.95], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 19368, 'image_id': 5653, 'category_id': 55, 'segmentation': [], 'area': 21922.602, 'bbox': [25.8, 16.1, 206.7, 106.06], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 19370, 'image_id': 5655, 'category_id': 55, 'segmentation': [], 'area': 20331.8208, 'bbox': [20.28, 59.7, 186.24, 109.17], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 19383, 'image_id': 5663, 'category_id': 55, 'segmentation': [], 'area': 19247.329999999998, 'bbox': [25.2, 64.6, 139.0, 138.47], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 19439, 'image_id': 5692, 'category_id': 55, 'segmentation': [], 'area': 481068.81740000006, 'bbox': [136.7, 161.42, 805.19, 597.46], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 19441, 'image_id': 5693, 'category_id': 55, 'segmentation': [], 'area': 429824.95, 'bbox': [219.1, 154.6, 727.9, 590.5], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 19702, 'image_id': 5827, 'category_id': 55, 'segmentation': [], 'area': 4460012.2119, 'bbox': [70.62, 70.98, 2902.69, 1536.51], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 19711, 'image_id': 5831, 'category_id': 55, 'segmentation': [], 'area': 20194.2117, 'bbox': [23.67, 62.5, 143.13, 141.09], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 19717, 'image_id': 5834, 'category_id': 55, 'segmentation': [], 'area': 25108.198799999995, 'bbox': [16.1, 34.95, 275.43, 91.16], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 19737, 'image_id': 5844, 'category_id': 55, 'segmentation': [], 'area': 20839.6071, 'bbox': [67.06, 33.43, 144.81, 143.91], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 19750, 'image_id': 5851, 'category_id': 55, 'segmentation': [], 'area': 29884.0128, 'bbox': [28.29, 15.88, 134.88, 221.56], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 19756, 'image_id': 5853, 'category_id': 55, 'segmentation': [], 'area': 13592.4366, 'bbox': [50.13, 58.82, 133.22, 102.03], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 19796, 'image_id': 5866, 'category_id': 55, 'segmentation': [], 'area': 15131.5815, 'bbox': [27.81, 69.51, 164.85, 91.79], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 19807, 'image_id': 5871, 'category_id': 55, 'segmentation': [], 'area': 23484.357599999996, 'bbox': [24.29, 47.24, 176.88, 132.77], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 19840, 'image_id': 5886, 'category_id': 55, 'segmentation': [], 'area': 15487.183799999999, 'bbox': [309.66, 177.28, 123.09, 125.82], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 19842, 'image_id': 5886, 'category_id': 55, 'segmentation': [], 'area': 15236.0802, 'bbox': [172.6, 24.62, 123.09, 123.78], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 19844, 'image_id': 5886, 'category_id': 55, 'segmentation': [], 'area': 15488.414699999998, 'bbox': [448.05, 23.27, 123.09, 125.83], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 19846, 'image_id': 5886, 'category_id': 55, 'segmentation': [], 'area': 15487.183800000006, 'bbox': [173.26, 177.28, 123.09, 125.82], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 30619, 'image_id': 8999, 'category_id': 60, 'segmentation': [], 'area': 9920.0, 'bbox': [0.0, 0.6, 100.0, 99.2], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 30626, 'image_id': 9003, 'category_id': 60, 'segmentation': [], 'area': 9929.061000000002, 'bbox': [0.0, 0.41, 99.9, 99.39], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 30629, 'image_id': 9005, 'category_id': 60, 'segmentation': [], 'area': 9949.062, 'bbox': [0.31, 0.0, 99.69, 99.8], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 30631, 'image_id': 9006, 'category_id': 60, 'segmentation': [], 'area': 9928.1271, 'bbox': [0.31, 0.41, 99.69, 99.59], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 30633, 'image_id': 9007, 'category_id': 60, 'segmentation': [], 'area': 9970.02, 'bbox': [0.1, 0.0, 99.9, 99.8], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 30639, 'image_id': 9010, 'category_id': 60, 'segmentation': [], 'area': 9919.122000000001, 'bbox': [0.1, 0.2, 99.8, 99.39], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 30644, 'image_id': 9012, 'category_id': 60, 'segmentation': [], 'area': 9295.695, 'bbox': [0.0, 0.41, 99.9, 93.05], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 30647, 'image_id': 9014, 'category_id': 60, 'segmentation': [], 'area': 9212.778, 'bbox': [0.0, 3.89, 99.9, 92.22], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 30656, 'image_id': 9018, 'category_id': 60, 'segmentation': [], 'area': 8619.2256, 'bbox': [3.37, 3.07, 92.64, 93.04], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 30716, 'image_id': 9031, 'category_id': 60, 'segmentation': [], 'area': 9920.12, 'bbox': [0.1, 0.4, 99.8, 99.4], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 30718, 'image_id': 9032, 'category_id': 60, 'segmentation': [], 'area': 9890.1, 'bbox': [0.1, 0.8, 99.9, 99.0], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 30762, 'image_id': 9042, 'category_id': 60, 'segmentation': [], 'area': 333239.87, 'bbox': [0.39, 0.0, 499.61, 667.0], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 19376, 'image_id': 5658, 'category_id': 55, 'segmentation': [], 'area': 33962.1336, 'bbox': [3.35, 36.54, 216.54, 156.84], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 17543, 'image_id': 5167, 'category_id': 40, 'segmentation': [], 'area': 7280.76, 'bbox': [14.2, 39.6, 87.72, 83.0], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 30625, 'image_id': 9002, 'category_id': 60, 'segmentation': [], 'area': 9890.1, 'bbox': [0.1, 0.8, 99.9, 99.0], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 19366, 'image_id': 5652, 'category_id': 55, 'segmentation': [], 'area': 21750.272, 'bbox': [18.54, 72.29, 147.76, 147.2], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 30617, 'image_id': 8998, 'category_id': 60, 'segmentation': [], 'area': 115025.83840000001, 'bbox': [0.0, 0.7, 339.68, 338.63], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 30623, 'image_id': 9001, 'category_id': 60, 'segmentation': [], 'area': 5041.567999999999, 'bbox': [24.64, 0.4, 50.72, 99.4], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 19446, 'image_id': 5695, 'category_id': 55, 'segmentation': [], 'area': 392298.77339999995, 'bbox': [20.39, 387.91, 681.82, 575.37], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 19730, 'image_id': 5841, 'category_id': 55, 'segmentation': [], 'area': 44611.454000000005, 'bbox': [4.02, 2.54, 214.85, 207.64], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 30621, 'image_id': 9000, 'category_id': 60, 'segmentation': [], 'area': 9940.08, 'bbox': [0.1, 0.2, 99.8, 99.6], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 19732, 'image_id': 5842, 'category_id': 55, 'segmentation': [], 'area': 23679.3555, 'bbox': [7.01, 8.97, 193.95, 122.09], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 2833, 'image_id': 765, 'category_id': 19, 'segmentation': [], 'area': 32626.7644, 'bbox': [54.12, 32.31, 147.88, 220.63], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 19618, 'image_id': 5785, 'category_id': 55, 'segmentation': [], 'area': 27174.465, 'bbox': [4.69, 50.8, 214.75, 126.54], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 19357, 'image_id': 5647, 'category_id': 55, 'segmentation': [], 'area': 22131.416399999995, 'bbox': [21.34, 49.52, 176.36, 125.49], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 19741, 'image_id': 5846, 'category_id': 55, 'segmentation': [], 'area': 27026.2542, 'bbox': [1.3, 2.3, 222.42, 121.51], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 8250, 'image_id': 2402, 'category_id': 33, 'segmentation': [], 'area': 24606.238799999992, 'bbox': [276.48, 154.52, 163.02, 150.94], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 8254, 'image_id': 2402, 'category_id': 33, 'segmentation': [], 'area': 28366.282100000008, 'bbox': [78.45, 534.9, 171.47, 165.43], 'iscrowd': 0, 'attributes': {'occluded': False}}\n",
            "{'id': 8258, 'image_id': 2402, 'category_id': 33, 'segmentation': [], 'area': 27345.361999999983, 'bbox': [277.69, 725.69, 170.27, 160.6], 'iscrowd': 0, 'attributes': {'occluded': False}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "R9ZwOlibvgxX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}